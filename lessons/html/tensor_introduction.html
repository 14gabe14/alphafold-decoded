<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/styles/default.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markdown-it-texmath/css/texmath.min.css">

<h1 class="lesson-heading" dir="auto" id="tensor-introduction">Tensor Introduction</h1>
<h2 class="lesson-sub-heading" dir="auto" id="intro">Intro</h2>
<p class="lesson-paragraph" dir="auto">In this lesson, we will give an Introduction to Tensors in PyTorch.
Tensors are the fundamental building block that drives Machine Learning.
Now, you have probably worked with tensors before. Vectors for example are one dimensional tensors. Matrices are two-dimensional tensors. Operations like the dot-product between vectors, or matrix vector multiplication, are classic examples of operations on tensors.</p>
<p class="lesson-paragraph" dir="auto">Tensors are a generalization of this: A tensor is an n-dimensional array.
Going beyond two or three dimensions might sound a little odd or technical, but it actually comes up really naturally.</p>
<p class="lesson-paragraph" dir="auto">If you start of working with black-and-white images, like we will do when classifying handwritten digits in the tutorials, you are totally fine with two dimensions. A black and white image is basically a matrix of values between 0 for black and 1 for white. You can specify a row and a column and access the pixel value at this location.</p>
<p class="lesson-paragraph" dir="auto">If you think of a coloured image however, at each pixel, there already are three values: The amount of red, green, and blue at the pixel location. At each pixel, we have a feature vector consisting of three values. The whole image is a feature volume, a three-dimensional tensor of shape (H, W, C), where H is the number of rows, or the height of the image, W is the number of columns, or the width of the image, and C is the number of channels, in this case three. To get the redness of the top left pixel, we would index into the image like img[0, 0, 0].</p>
<p class="lesson-paragraph" dir="auto">But three dimensions often aren't enough: In Machine Learning, we are usually working with batches of data, for example, we might compute the forward pass through our model for a set of N images of different dogs. Now, we would need a 4-dimensional tensor of shape (N, H, W, C) to process all the images in a joint fashion. This is the typical form of tensors in machine learning: A batch of feature volumes, where the dimensions of the feature volume have actual semantic meaning (like the location in the image and the color channel) and the batch dimension is just being carried along in a 'for-each' manner.</p>
<p class="lesson-paragraph" dir="auto">So, this is what tensors are, but what is PyTorch? PyTorch is a Machine Learning framework, and at the core, it has two tasks:
First, it needs to implement a multitude of operations on tensors, like matrix multiplication, and it needs to do so very efficiently. You may have worked with numpy before, which basically does the same thing. PyTorch however can be orders of magnitudes faster, as it can harness the power of your Graphics Processing Unit.
The second task is a little less obvious, and it's called Automatic Differentiation. We will take a closer look at why this is so important in our Intro to machine learning, but basically, training a machine learning model is about minimizing the error of your model. And the way that this is done is by computing the derivative of the error with respect to every single parameter in your model. Knowing the derivative, we know in which direction we need to change the parameter to decrease the error, and this is the whole magic of Machine Learning. Calculating these derivatives by hand is cumbersome and error-prone, and luckily, PyTorch does it automatically.
[We won't be training AlphaFold ourselves in this series, so aside of the Introduction to Machine Learning, you will probably only see this feature coming up as a spike in your used memory when you enable the feature by accident, as keeping track of the whole computational graph for the calculation of derivatives requires a lot of compute resources.]</p>
<p class="lesson-paragraph" dir="auto">~ 3 Minutes</p>
<h2 class="lesson-sub-heading" dir="auto" id="tensor-creation">Tensor Creation</h2>
<p class="lesson-paragraph" dir="auto">So, now that we know what tensors are, let's create one. The simplest method for tensor creation is <code class="lesson-code">torch.tensor</code>. It takes a nested list as an argument and creates a tensor of the same shape. Most of the time, you will use this syntax to create a vector or a matrix. This, for example, is a 4-element vector: <code class="lesson-code">torch.tensor([2.0, 1.45,-1.0, 0.0])</code>. This would be a 3x2 matrix: <code class="lesson-code">torch.tensor([[1,2],[1,2],[3,4]])</code>. You can see that the outer pair of brackets has three elements, while the inner ones have two each. This is why the tensor has shape (3, 2), and not (2, 3). You can use this syntax to create any shape of tensor. This for example, would be a 4x1 tensor: <code class="lesson-code">torch.tensor([[2.0],[1.45],[-1.0],[0.0]])</code>. This concept of one-dimensions is really important. Even though this tensor has the same values as the one-dimensional vector we have seen earlier, it has significantly different properties when used in tensor operations.
There are some other important tensor creation routines, that we'll get to know better in the tutorials. <code class="lesson-code">torch.linspace</code> for example is a method that is used very often. It creates evenly spaced values between a specified start and end. <code class="lesson-code">torch.linspace(0, 2, steps=5)</code> would create the tensor <code class="lesson-code">[0, 0.5, 1, 1.5, 2]</code>. Another important method is <code class="lesson-code">torch.ones</code>, which takes a desired shape, and creates a tensor of this shape where every value is 1.0. You can scale this tensor to any other value. This line creates a 4x3x4 tensor of 5s: <code class="lesson-code">torch.ones((4,3,4)) * 5</code></p>
<p class="lesson-paragraph" dir="auto">Tensors support most basic operations, like + or -, but also logical operations like &gt;, &lt; or ==. For example, the comparison of these two vectors - <code class="lesson-code">torch.tensor([3,5,1]) &lt; torch.tensor([4, 5, 8])</code> - returns the boolean tensor [True, False, True]. As we can see here, tensors can have different datatypes, like <code class="lesson-code">long</code> for whole numbers, <code class="lesson-code">float</code> for fractional numbers and <code class="lesson-code">bool</code> for booleans. You can cast a tensor to a different type. We can cast our boolean vector back to floats for example, which would return [1.0, 0.0, 1.0].</p>
<p class="lesson-paragraph" dir="auto">~ 2 Minutes</p>
<h2 class="lesson-sub-heading" dir="auto" id="tensor-manipulation">Tensor Manipulation</h2>
<p class="lesson-paragraph" dir="auto">One of the most important concepts for tensor manipulation is understanding the shapes of tensors, and how we change them. To properly understand reshaping, we need to understand how a tensor is stored in computer memory. Although we are defining tensors with a specific, n-dimensional shape, in reality, all tensors are one-dimensional: A sequence of all the values in the tensor. If we have a 3x4 matrix for example, this is a 12-element one dimensional tensor. And understanding how reshaping works is understanding how this flattening process works.</p>
<p class="lesson-paragraph" dir="auto">By default, PyTorch uses a row-major order. This means that elements that are next to each other in a row are also next to each other in the flattened, 1D version of the tensor. The 1D version is constructed by taking all the rows, and concatenating them. This also means that the element one row further down has an offset of W to our element, where W is the number of columns of the matrix.</p>
<p class="lesson-paragraph" dir="auto">This generalizes to more than two dimensions: Let's say we have a batch of matrices, with shape (N, H, W). Flattening this tensor means flattening all individual matrices, which will have shape (H*W) when flattened, and then concatenating them. Regarding the order of the dimensions (batch, rows, columns), flattening happens from right to left: We first go through all the different columns before jumping to the next row. For that we go through all the columns again, go to the next row, and so on. After we are done flattening all the rows, we go to the next matrix in the batch.</p>
<p class="lesson-paragraph" dir="auto">A proper understanding of this flattening process makes it easy to understand the concept of reshaping: Reshaping means going to the flattened version of a tensor, then changing the subdivision.
Let's look at an example:
<code class="lesson-code">torch.tensor([[0, 1, 2], [3, 4, 5]])</code>
This is a 2x3 tensor. It's flattened version looks like this:
<code class="lesson-code">[0, 1, 2, 3, 4, 5]</code>
If we are reshaping it to shape 3x2 now, this means that we are only taking two elements for each row, so the rows are
<code class="lesson-code">[0, 1], [2, 3,], [4, 5]</code>
The full tensor now looks like this:
<code class="lesson-code">[[0, 1], [2, 3], [4, 5]]</code>
This use of &quot;Rearrangement Reshaping&quot; is not the most common. More often, we will use reshape to flatten some dimensions, like reshaping (N, H, W) to (N, H*W), or for unflattening, which is just the reverse. Another common use-case for reshaping is adding 1-dimensions.</p>
<p class="lesson-paragraph" dir="auto">~ 2:30 Minutes</p>
<p class="lesson-paragraph" dir="auto">Another fundamental task when working with tensors is <em>indexing</em> and <em>slicing</em>. Indexing is about accessing an element at a specific position in the tensor. If we have this matrix for example
<code class="lesson-code">A = torch.tensor([[0,1,2],[3,4,5]])</code>
and we wanted to access the element 3, we would index to it using <code class="lesson-code">A[1, 0]</code>, as it is in the second row (index 1) and first column (index 0). One thing to note is that when indexing elements in pytorch, you don't directly get the elements as numbers, but as one-element tensors. This is because PyTorch wants to keep track of values in its specific format, for example to allow for automatic differentiation. If you want to access the actual value as a number, you will need to write this as <code class="lesson-code">A[2, 0].item()</code>.</p>
<p class="lesson-paragraph" dir="auto">Slicing lets you extract specific portions of a tensor. Let's look at an example we'll meet again in AlphaFold:
In the Structure Module, we will come across the need to represent transforms, 3D motions, which consist of a Rotation and a Translation. They are in the format of 4x4 matrices of the following form:</p>
<section><eqn><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>T</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.1600em" columnalign="center center" columnlines="solid" columnspacing="1em" rowlines="solid"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>R</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>t</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn><mtext>  </mtext><mn>0</mn><mtext>  </mtext><mn>0</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mtext>  </mtext><mover accent="true"><mi>x</mi><mo>~</mo></mover><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.1600em" columnalign="center" columnspacing="1em" rowlines="solid"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">T = \left(\begin{array}{c|c} R &amp; t \\ \hline 0\;0\;0 &amp; 1\end{array}\right), \; \tilde{x} = \begin{pmatrix}x\\ \hline 1 \end{pmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.45em;"><span class="pstrut" style="height:3.45em;"></span><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="vertical-separator" style="height:2.4em;border-right-width:0.04em;border-right-style:solid;margin:0 -0.02em;vertical-align:-0.95em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">t</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span style="top:-3.7em;"><span class="pstrut" style="height:3.45em;"></span><span class="hline" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">~</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.45em;"><span class="pstrut" style="height:3.45em;"></span><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-3.61em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-2.4099999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span></span></span><span style="top:-3.7em;"><span class="pstrut" style="height:3.45em;"></span><span class="hline" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em;"><span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span></span></span></span></span></eqn></section></math>
<p class="lesson-paragraph" dir="auto">Here, <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span></eq> is a 3x3 matrix, and <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">t</span></span></span></span></eq> is a 3-element vector. If we want to crop the 3x3 matrix from this transform, we would index it as
<code class="lesson-code">R = T[0:3, 0:3]</code>, to specify that for the rows and columns, we want to go from index 0 (inclusive) to index 3 (exclusive). Starting at 0 is the default, so we could rewrite this as <code class="lesson-code">R = T[:3, :3]</code>.
For the translation, we want the first 3 rows, but only the fourth column. We can write this as <code class="lesson-code">t = T[:3, 3]</code>. As the fourth row is the last row, we could also write this as <code class="lesson-code">t = T[:3, -1]</code>, using negative indexing. With this syntax, t would be a one-dimensional tensor. But we might want to concatenate <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span></eq> and <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathnormal">t</span></span></span></span></eq> again later into a 3x4 tensor. For this, R and t would need the same number of dimensions, R would be a 3x3 tensor and t a 3x1 tensor. For this, we could also use slicing in the following way: <code class="lesson-code">t = T[:3, 3:4]</code>, or <code class="lesson-code">t=T[:3, 3:]</code>, as going to the end is the default. This pattern of using one-element slices, like 3:4, is common to conserve one-dimensions.</p>
<p class="lesson-paragraph" dir="auto">Now, we might not work with a single transform <code class="lesson-code">T</code> of shape (4, 4), but with a batch of trasforms of shape (N, 4, 4). Or we might even have one transform for each backbone in the batch, so we would have shape (N, N_res, 4, 4), where N_res is the number of residues. To account for this, PyTorch provides the ellipsis operator <code class="lesson-code">...</code> which is really useful. It expands to as many <code class="lesson-code">:</code> as are necessary to match the dimensions. The syntax <code class="lesson-code">R = T[..., :3, :3]</code> correctly extracts the rotation matrices. For the batched case, <code class="lesson-code">...</code> would expand to <code class="lesson-code">:</code>. For the double-batched case, it would be <code class="lesson-code">:, :</code> and for non-batch use, it's omitted.</p>
<p class="lesson-paragraph" dir="auto">As a last note on indexing, we will talk on left-hand side and right-hand side indexing. The terms left-hand side and right-hand side refer to the left and right side of an equal sign in programming. So far, we have only used right-hand side indexing, where we index to access a slice of a tensor, and assign it to a variable or process it further.</p>
<p class="lesson-paragraph" dir="auto">Left-hand side indexing works just as well: Let's say we have a 3x3 tensor <code class="lesson-code">A</code>, and we want to replace it's middle column with the tensor [0, 1, 2]. We  can do that like this:</p>
<p class="lesson-paragraph" dir="auto"><code class="lesson-code">A[:, 1] = torch.tensor([0, 1, 2])</code></p>
<p class="lesson-paragraph" dir="auto">Here, we used left-hand side indexing to select the slice of <code class="lesson-code">A</code> and assign new values to it. This also works with all other indexing techniques.</p>
<p class="lesson-paragraph" dir="auto">~ 3:45 Minutes</p>
<p class="lesson-paragraph" dir="auto">----------- Part One -----------</p>
<p class="lesson-paragraph" dir="auto">Topics for Part 2: Computations and reductions along axes, broadcasting, torch.einsum</p>
<p class="lesson-paragraph" dir="auto">Welcome to part 2 of our Introduction to Tensors! In the first part, we covered many of the basics regarding tensors, like the creation of tensors, indexing and reshaping. In this part, we want to highlight some more advanced concepts, in particular computations along axes, broadcasting and the einsum method.</p>
<h2 class="lesson-sub-heading" dir="auto" id="computations-along-axes">Computations Along Axes</h2>
<p class="lesson-paragraph" dir="auto">Let's start with computations along axes. Say you have got a 4x3 matrix and you want to sum up the elements. You have three  different options: Summing up all elements (resulting in a 1-element tensor), summing up all the rows (resulting in a 3-element tensor) or summing up all the colunmns (resulting in a 4-element tensor). You can specify the behaviour by setting the <code class="lesson-code">dim</code> argument in the method <code class="lesson-code">torch.sum</code>. If set to <code class="lesson-code">dim=0</code>, summation will happen along the row dimension. If set to <code class="lesson-code">dim=1</code> summation will happen along the column dimension. If set to <code class="lesson-code">None</code>, all elements will be summed up. You can also set it to a tuple: Let's say you had a batch of matrices, of shape (N, 4, 3). You can calculate the sum of all elements for the matrices individually as <code class="lesson-code">torch.sum(A, dim=(1, 2))</code> or <code class="lesson-code">torch.sum(A, dim=(-1,-2))</code> using negative indexing, resulting in an N-element vector.</p>
<p class="lesson-paragraph" dir="auto">There are many such operations: With <code class="lesson-code">torch.argmax</code> you can compute the index of the largest element (total or per row/column), with <code class="lesson-code">torch.mean</code> and <code class="lesson-code">torch.std</code> you can compute the mean and standard deviation, and with <code class="lesson-code">torch.linalg.vector_norm</code> you can compute the standard L2-norm along a dimension. All of these examples are reducing, that means that the dimension you specified as <code class="lesson-code">dim</code> will be missing in the output shape. You can set the parameter <code class="lesson-code">keepdim=True</code> to keep it as a one-dimension. This can be really useful, for example to allow for concatenation or broadcasting.</p>
<p class="lesson-paragraph" dir="auto">There are also computations along axes that aren't reducing. One that we will use in the tutorials is the softmax function. The sofmax function takes a vector <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\bold{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.44444em;vertical-align:0em;"></span><span class="mord mathbf">x</span></span></span></span></eq> and creates a new vector according to</p>
<section><eqn><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">softmax</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mi>exp</mi><mo>⁡</mo><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\operatorname{softmax}(x)_i = \frac{\exp{x_i}}{\sum_j\exp{x_j}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop"><span class="mord mathrm">softmax</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.229378em;vertical-align:-1.1218180000000002em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1075599999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1218180000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></eqn></section></math>
<p class="lesson-paragraph" dir="auto">It has three important properties:</p>
<ul dir="auto">
<li class="lesson-li" dir="auto">The entries sum up to 1, so the results can be interpreted as a probability distribution</li>
<li class="lesson-li" dir="auto">If one value in the input is significantly larger than the others (say larger by ~6), the result will be close to a one-hot vector where only this entry is 1 and the others are 0, since the exponential function boosts large values overproportionally strong</li>
<li class="lesson-li" dir="auto">In comparison to a 'hard' max function that sets the largest value to 1 and all others to 0, it distributes the mass more smoothly if the large values are close to each other.</li>
</ul>
<p class="lesson-paragraph" dir="auto">Even though the softmax function is no reduction, it needs a specified dimension, as it is a vector-to-vector calculation. Along the specified dimension, it will create values that sum up to one. Along the other dimensions, it doesn't have this property.</p>
<p class="lesson-paragraph" dir="auto">Another similar case where you need to think about axes is when concatenating or stacking tensors. Let's say you have three 4-element tensors <code class="lesson-code">u</code>, <code class="lesson-code">v</code>, and <code class="lesson-code">w</code>. You can use <code class="lesson-code">torch.stack</code> to stack them to a matrix. If you stack them as <code class="lesson-code">torch.stack((u, v, w), dim=0)</code>, you will get a 3x4 matrix where the vectors are the individual rows. If you use <code class="lesson-code">torch.stack((u, v, w), dim=1)</code> or <code class="lesson-code">torch.stack((u, v, w), dim=-1)</code>, you will get a 4x3 matrix with the vectors as the individual columns. The position of the newly introduced dimension 3 is given by <code class="lesson-code">dim</code>. For <code class="lesson-code">torch.stack</code>, all tensors to be stacked need to have the same shape, so that the resulting tensor has block form. The similar <code class="lesson-code">torch.cat</code> doesn't introduce a new dimension but glues the tensors together along an existing dimension. We've mentioned before, when we extracted the 3x3 matrix <code class="lesson-code">R</code> and the translation vector <code class="lesson-code">t</code> from the 4x4 tensor <code class="lesson-code">T</code>, that it is beneficial to extract <code class="lesson-code">t</code> with slicing so that it has shape (3, 1) instead of (3,). This is practical here: If we wanted to concatenate <code class="lesson-code">R</code> and <code class="lesson-code">t</code> again, we can do so using <code class="lesson-code">torch.cat((R, t), dim=-1)</code> since they have the same number of dimensions.</p>
<h2 class="lesson-sub-heading" dir="auto" id="broadcasting">Broadcasting</h2>
<p class="lesson-paragraph" dir="auto">We have hinted on broadcasting when talking about the importance of one-dimensions. Here, we will finally see what all the fuzz is about.</p>
<p class="lesson-paragraph" dir="auto">In it's simplest form, broadcasting follows this idea: Let's say we have a 4x3 matrix <code class="lesson-code">A</code> and a vector <code class="lesson-code">B = torch.tensor([1, 2, 3])</code>. <code class="lesson-code">B</code> has the same number of elements as the rows of <code class="lesson-code">A</code>, and we might want to add it to each row of <code class="lesson-code">A</code>. To do so, we would need to broadcast it from it's current shape <code class="lesson-code">(3,)</code> to the shape <code class="lesson-code">(4, 3)</code>, i.e. to <code class="lesson-code">torch.tensor([[1,2,3],[1,2,3],[1,2,3],[1,2,3]])</code>. Broadcasting in PyTorch allows us to do just that. First, we would introduce a new one-dimension to <code class="lesson-code">B</code> and get it to shape (1, 3). This means interpreting the vector <code class="lesson-code">B</code> as a matrix with one row. We have several options to do so: <code class="lesson-code">B=B[None, :]</code>, <code class="lesson-code">B = B.unsqueeze(0)</code>, or <code class="lesson-code">B = B.reshape(1, 3)</code> are all equally effective in creating the new shape for <code class="lesson-code">B</code>. Now, if we try to add <code class="lesson-code">A</code> and <code class="lesson-code">B</code>, PyTorch automatically broadcasts <code class="lesson-code">B</code> to shape (4, 3) by duplicating the row.</p>
<p class="lesson-paragraph" dir="auto">If we had <code class="lesson-code">B = torch.tensor([1,2,3,4])</code> instead, we could do the same trick to add <code class="lesson-code">B</code> to all columns of <code class="lesson-code">A</code>. First, we'd compute <code class="lesson-code">B = B.reshape(4, 1)</code> so that it is a matrix with one column, and when computing <code class="lesson-code">A + B</code> now, it is implicitly broadcasted to shape (4, 3) to match the shape of <code class="lesson-code">A</code>.</p>
<p class="lesson-paragraph" dir="auto">In general, the rules for broadcasting are the following:</p>
<ul dir="auto">
<li class="lesson-li" dir="auto">the dimensions of A and B are aligned with each other. Here, this would mean the following: <section><eqn><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.1600em" columnalign="center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>A</mi><mo>:</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>4</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">∣</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>B</mi><mo>:</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>3</mn></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{matrix}A: &amp; 4 &amp; 3 \\ &amp; | &amp; | \\B: &amp; 1 &amp; 3 \\ \end{matrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.6000000000000005em;vertical-align:-1.5500000000000007em;"></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span></span></span></span></span></eqn></section></li>
<li class="lesson-li" dir="auto">to be broadcastable, the individual dimensions must be equal, or set to one for one of the tensors</li>
<li class="lesson-li" dir="auto">along these one-dimensions, the tensor is expanded by duplicating it, until it matches the shape of the other tensor</li>
</ul>
<p class="lesson-paragraph" dir="auto">There actually is one more rule:</p>
<ul dir="auto">
<li class="lesson-li" dir="auto">dimensions are aligned from right to left. If one tensor has fewer dimensions than the other, ones are prepended to match the number of dimensions</li>
</ul>
<p class="lesson-paragraph" dir="auto">With this last rule, we could have left out the reshaping when adding <code class="lesson-code">B</code> as a row vector to <code class="lesson-code">A</code>. The shapes (4, 3) and (3,) are broadcastable, as they are aligned from left to right, and ones are prepended to match the number of dimensions.</p>
<p class="lesson-paragraph" dir="auto">Let's look at another example: Say we have a batch V of vectors, a tensor of shape (N, 3), and we wanted to normalize them. We can compute the norms for each vector by using <code class="lesson-code">norms = torch.linalg.vector_norm(V, dim=-1)</code>. The result is of shape (N,). We can unsqueeze it with <code class="lesson-code">norms=norms.reshape(N, 1)</code>. Now, it is broadcastable and we can compute
<code class="lesson-code">V_normalized = V / norms</code>.
However, we could have saved the intermediate step by using <code class="lesson-code">keepdim</code>:
<code class="lesson-code">V_normalized = V / torch.linalg.vector_norm(V, dim=-1, keepdim=True)</code>.
This way, the calculated norms directly have shape (N, 1) and are broadcastable against <code class="lesson-code">V</code>.</p>
<p class="lesson-paragraph" dir="auto">Another usecase for broadcasting are 'each-with-each' operations. Let's say we have a 3-element vector <code class="lesson-code">v</code> and a 2 element vector <code class="lesson-code">w</code>. If we want to compute the product of each element of <code class="lesson-code">v</code> with each element of <code class="lesson-code">w</code>, we would get six elements in total. We can organize them in a 3x2 matrix, where the entry i,j is made up of <code class="lesson-code">v[i] * w[j]</code>.</p>
<section><eqn><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">(</mo><mtable rowspacing="0.1600em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>v</mi><mn>1</mn></msub><msub><mi>w</mi><mn>1</mn></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>v</mi><mn>1</mn></msub><msub><mi>w</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>v</mi><mn>2</mn></msub><msub><mi>w</mi><mn>1</mn></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>v</mi><mn>2</mn></msub><msub><mi>w</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>v</mi><mn>3</mn></msub><msub><mi>w</mi><mn>1</mn></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>v</mi><mn>3</mn></msub><msub><mi>w</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.1600em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>2</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>3</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>3</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>∗</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.1600em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>2</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>2</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>w</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{pmatrix}v_1w_1 &amp; v_1w_2 \\ v_2w_1 &amp; v_2w_2 \\ v_3w_1 &amp; v_3w_2\end{pmatrix} = \begin{pmatrix}v_1 &amp; v_1 \\ v_2 &amp; v_2 \\ v_3 &amp; v_3\end{pmatrix} * \begin{pmatrix}w_1 &amp; w_2 \\ w_1 &amp; w_2 \\ w_1 &amp; w_2\end{pmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span style="top:-3.3970000000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span style="height:0.016em;width:0.875em;"><svg width='0.875em' height='0.016em' style='width:0.875em' viewBox='0 0 875 16' preserveAspectRatio='xMinYMin'><p class="lesson-paragraph"ath d='M291 0 H417 V16 H291z M291 0 H417 V16 H291z'/></svg></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span style="top:-3.3970000000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span style="height:0.016em;width:0.875em;"><svg width='0.875em' height='0.016em' style='width:0.875em' viewBox='0 0 875 16' preserveAspectRatio='xMinYMin'><p class="lesson-paragraph"ath d='M457 0 H583 V16 H457z M457 0 H583 V16 H457z'/></svg></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span style="top:-3.3970000000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span style="height:0.016em;width:0.875em;"><svg width='0.875em' height='0.016em' style='width:0.875em' viewBox='0 0 875 16' preserveAspectRatio='xMinYMin'><p class="lesson-paragraph"ath d='M291 0 H417 V16 H291z M291 0 H417 V16 H291z'/></svg></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span style="top:-3.3970000000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span style="height:0.016em;width:0.875em;"><svg width='0.875em' height='0.016em' style='width:0.875em' viewBox='0 0 875 16' preserveAspectRatio='xMinYMin'><p class="lesson-paragraph"ath d='M457 0 H583 V16 H457z M457 0 H583 V16 H457z'/></svg></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.60004em;vertical-align:-1.55002em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span style="top:-3.3970000000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span style="height:0.016em;width:0.875em;"><svg width='0.875em' height='0.016em' style='width:0.875em' viewBox='0 0 875 16' preserveAspectRatio='xMinYMin'><p class="lesson-paragraph"ath d='M291 0 H417 V16 H291z M291 0 H417 V16 H291z'/></svg></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.21em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.0099999999999993em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-1.8099999999999994em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.5500000000000007em;"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05002em;"><span style="top:-2.2500000000000004em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span style="top:-3.3970000000000002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span style="height:0.016em;width:0.875em;"><svg width='0.875em' height='0.016em' style='width:0.875em' viewBox='0 0 875 16' preserveAspectRatio='xMinYMin'><p class="lesson-paragraph"ath d='M457 0 H583 V16 H457z M457 0 H583 V16 H457z'/></svg></span></span><span style="top:-4.05002em;"><span class="pstrut" style="height:3.1550000000000002em;"></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55002em;"><span></span></span></span></span></span></span></span></span></span></span></span></eqn></section></math>
<p class="lesson-paragraph" dir="auto">This is called the outer product of <code class="lesson-code">v</code> and <code class="lesson-code">w</code>. We can calculate this using broadcasting, by reshaping <code class="lesson-code">v</code> to a 3x1 column vector and <code class="lesson-code">w</code> to a 1x2 row vector, followed by a broadcasted multiplication of the two.</p>
<h2 class="lesson-sub-heading" dir="auto" id="torcheinsum">torch.einsum</h2>
<p class="lesson-paragraph" dir="auto">Broadcasting is an incredibly powerful tool. Together with the indexing, reshaping and reduction techniques we have seen so far, we already have sufficient tools to solve almost any problem in the whole series.</p>
<p class="lesson-paragraph" dir="auto">If you try to do so however, you will see that these methods can quickly get a little cumbersome. Let's look at the most classic tensor-tensor operation: Matrix multiplication. Given two matrices <code class="lesson-code">A</code> of shape (i,k) and <code class="lesson-code">B</code> of shape (k,j), the matrix product can formulated like this:</p>
<p class="lesson-paragraph" dir="auto">For each row of <code class="lesson-code">A</code> and for each column of <code class="lesson-code">B</code>, calculate the pointwise product of the row and the column. After that, sum up along the row-dimension of <code class="lesson-code">A</code> (which is the column dimension of <code class="lesson-code">B</code>)</p>
<p class="lesson-paragraph" dir="auto">We have seen before that 'each-with-each' calculations can be computed by expanding with one-dimensions and doing a broadcasted multiplication. For the first step of this problem, we would calculate <code class="lesson-code">C = A.reshape(i, k, 1) * B.reshape(1, k, j)</code>. The result would be a tensor of shape (i,k,j). The slices <code class="lesson-code">C[i, :, j]</code> consist of the pointwise multiplication of the i-th row of <code class="lesson-code">A</code> with the j-th column of <code class="lesson-code">B</code>. For matrix multiplication, we would calculate <code class="lesson-code">C = torch.sum(C, dim=1)</code> to sum these elements up.</p>
<p class="lesson-paragraph" dir="auto">We might also want to compute batched matrix multiplication, where <code class="lesson-code">A</code> has shape (N, i, k) and <code class="lesson-code">B</code> has shape (N, k, j). We could compute this as
<code class="lesson-code">C = torch.sum(A.reshape(N, i, k, 1) * B.reshape(N, 1, k, j), dim=-2)</code></p>
<p class="lesson-paragraph" dir="auto">In this example, we see all typical elements of a tensor-tensor operation: Some of the dimensions (i and j) are present in one of the educts but not the other, and are worked on in an 'each-with-each' fashion. One dimension (k) is present in both and aligned. After calculating the product of the aligned vectors, the dimension is contracted by summation. One dimension (N) is present in both and is just used for a batched, 'for-each' calculation.</p>
<p class="lesson-paragraph" dir="auto">PyTorch has a method to cover all these cases in a really concise manner, by using the Einstein notation with <code class="lesson-code">torch.einsum</code>. The method takes an equation string and the tensors that are used in the operation. For our case of matrix multiplication, the operation would be <code class="lesson-code">torch.einsum('ik,kj-&gt;ij', A, B)</code>. The batched matrix multiplication would be computed as <code class="lesson-code">torch.einsum('Nik,Nkj-&gt;Nij', A, B)</code>. The equation string directly follows the rules we have seen above:</p>
<ul dir="auto">
<li class="lesson-li" dir="auto">if a letter is present in one of the educts, but not in the other and kept in the product (like i and j), the other tensor is unsqueezed and broadcasted to include the dimension</li>
<li class="lesson-li" dir="auto">if a letter is present in both educts, but not in the product (like k), it is contracted</li>
<li class="lesson-li" dir="auto">if a letter is present in both educts and the product, it is treated as a batched, 'for-each' dimension</li>
</ul>
<p class="lesson-paragraph" dir="auto">In the same way we used the ellipsis operator before during indexing, we can use it here as well to account for any number (or none at all) of prepended batch dimensions. With <code class="lesson-code">torch.einsum('...ik,...kj-&gt;...ij', A, B)</code> we can cover both the batched and non-batched case simultaneously.</p>
<p class="lesson-paragraph" dir="auto">Another example is the outer product we computed earlier. Without explicitly reshaping the tensors ourselves, it can be written as <code class="lesson-code">torch.einsum('i,j-&gt;ij', v, w)</code>.</p>
<p class="lesson-paragraph" dir="auto"><code class="lesson-code">torch.einsum</code> is incredibly flexible. You will use it all the time throughout the series, and you will already see a lot of examples in the tutorial notebook for this tensor introduction. The construction of the equation string can be a bit tricky at the start, but you will quickly get used to it.</p>
<h2 class="lesson-sub-heading" dir="auto" id="conclusion">Conclusion</h2>
<p class="lesson-paragraph" dir="auto">With this, we are done with the introduction to tensors. In total, this was a quite extensive introduction, but it has the benefit that you've already seen almost all of the operations you will need for the whole series. There are some more miscallaneous operations we will need on the way, but with all the tools discussed in here, you have enough foundation to look up new methods online or with ChatGPT. We have prepared a tutorial Jupyter Notebook where you are guided through using all the methods we have shown you. After that, you will be well prepared to start with the next part of the series: The Introduction to Machine Learning.</p>
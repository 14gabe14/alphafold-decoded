# Attention 

In 2017, Google released a paper on its Transformer architecture for natural language processing, giving the paper the name "Attention is All You Need". So far, it lived up to its reputation. You'll have a hard time finding a modern breakthrough in machine learning that doesn't use Attention as its core mechanism. And, like most clever ideas in machine learning, it is actually a quite simple idea. 

Imagine trying to understand a complex sentence without knowing which words to focus on. This is where the game-changing concept of Attention in machine learning comes in. The attention mechanism addresses the problem of dealing with sequential data – which is most existing data: language is sequences of words, music is sequences of notes, and proteins are sequences of amino acids. The name attention comes from its idea to determine which tokens in the input are the most relevant for the task at hand, that is which tokens the model should pay attention to, before basing its prediction on these attention weights.

So far, we learned how to build machine learning models that solve vector-to-vector tasks. The network shown here is a visualization we've used in the last video to showcase a two-layer feed-forward neural network. It transforms a  784 element input vector, the image, to a 16-element intermediate, and finally to a 10-element output. We will simplify our visualization a little to be able to visualize more complicated combinations of these operations: The colored squares denote vectors, and the nodes are operations, or layers. This network uses a linear layer, going from a dimension of 784 to 16, a ReLU layer, and another linear layer from 16 to 10 elements. Just remember that these nodes aren't magic but concrete, simple operations: A linear layer means matrix multiplication, for example with a 784x16 matrix, sometimes with addition of a bias, and a ReLU layer sets all negative elements to zero. We'll also sometimes simplify and architecture like this, a stack of linear layers with non-linearities in between, and simply call it a feed-forward network.

The question now is, how do we use these primitives to work on sequences? Say we've got a long sentence of N words, and each word is encoded as a 128-element vector. This is a typical starting point in text processing: Words, or parts of words, are replaced by integers using a so-called tokenizer, and then each integer is replaced with a learned feature vector. If we wanted to use this phrase as input to a feed-forward machine learning model, we'd  basically have two options:

1. Concatenation: We concat all the N words to an N*128 element vector. This is called a fully-connected architecture, and it captures the full information, but has two drawbacks: First, the necessary weight matrix will get impossibly large for long sentences, and second, we don't have flexibility for input lengths, as the number of weights directly corresponds to the number of inputs.

2. Recursive Neural Networks: We keep track of a hidden state $h_t$, that is iteratively updated by the words. We start with an initial state $h_0$, for example $h_0=0$, and in each step, we concat $h_t$ with the input $x_t$ and feed it into our neural net to compute $h_{t+1}$:
    $$h_{t+1} = \text{model}(h_t, x_t)$$
    Note that we use the same model and the same weights for each timestep t. 
    After passing through all $x_t$, the final output $h_N$ was influenced by all of the words, and might be able to answer questions on them, like if the text was positive or negative, or if it contained the word "dog". This has the advantage of having a fixed number of parameters, independent of the input size, and that it can process arbitrarily long sequences. The major drawbacks are that $h_N$ has to summarize all the information on the whole sentence, which is a huge bottleneck, and that the encoding needs to be computed sequentially (because we need to know $h_t$ to compute $h_{t+1}$). 

To the rescue comes the attention mechanism. It follows a simple idea: Every token in the input gives a quick summary of its content. The model creates a question and checks it against each of the summaries, to give scores on how much they should contribute to the result. Then, each of the inputs is weighted according to its attention score to compute the result. 

Let's walk through this step-by-step. The "summaries" are so-called key vectors. Each input vector $x \in \mathbb{R}^c$ is mapped to a key vector by multiplication with a $d \times c$ matrix $K$. Here, $d$ is the embedding dimension of the attention mechanism. 

For the creation of the questions, or query vectors, we have several options. If we want to extract static information from the text, like the language its written in, if its positive or negative, or if its truethful, we can simply use a static, learned vector per query. These work just like the weight or bias parameters from the last chapter: We start with random vectors, calculate their gradients during training and update them by gradient descent.

<!-- For the creation of the questions, or query vectors, we have several options: For a task like machine translation, we could use a recursive neural network that goes through all words in the original language and all translated words that were generated so far, to generate a query vector for the next word. In Alphafold, we mostly use self-attention, where we simply generate one query vector from each input vector by multiplication with a $d \times c$ matrix $Q$. -->

To test how relevant the key is for the query, we compute the dot-product of the two. The dot product gives a score of how aligned the two vectors are: If they point in the same direction, the dot-product is the product of their magnitude. If they are orthogonal, the dot-product is zero, and if they point in opposite directions, the dot-product is the negative product of their magnitudes. With the flexibility of learned parameters for the creation of the key and query vectors, we don't need a more complicated "agreement" metric. However, we usually use a scaled dot-product $\frac{1}{\sqrt{d}} \bold{q}_i \cdot \bold{k}_j$ with a factor of $\frac{1}{\sqrt{d}}$ to make the computation more stable for different embedding sizes. 

So far, we got a matrix of raw attention scores $\tilde{a}{ij} = \frac{1}{\sqrt{d}} \bold{q}_i \cdot \bold{k}_j$ of shape (q, k), where q is the number of query vectors and k is the number of key vectors, which is the number of inputs. We want the attention scores $\tilde{a}{ij}$ for a fixed $i$ to be a distribution over the different inputs $\bold{x}_j$. For that we need some kind of normalization. The typical normalization is the softmax function we've seen before:
$$a_{ij} = \frac{\exp(\tilde{a}{ij})}{\sum_j \exp(\tilde{a}{ij})}$$

To generate the final outputs, we create a value vector $\bold{v}_j$ for each input by multiplication of $\bold{x}_j$ with a $v \times c$ matrix $V$. These value vectors are scaled by their attention weights and summed up to create one output per query vector $\bold{q}_i$:
$$\bold{o}i = \sum_j a{ij} \bold{v}_j$$

This is the full attention mechanism: We create key and value embeddings by multiplication of the inputs with key and value matrices $K$ and $V$. For a set of query vectors, we compute the key-query similarities as their scaled dot-product. We normalize the attention scores with softmax. Then, we weigh the value vectors by the attention scores and add them up. In the end, we get one value per query. 

There are a bunch of small variations on this core mechanism, and we'll look at some of them next. Our first idea is to change up how we create the query vectors. For many tasks, the questions we want to ask are directly linked to the inputs. Let's say for example, we wanted to do translation from english to spanish. To know which of the inputs is most relevant for the creation of the first output, we would already need an understanding of the text. To get such an input-dependent query, we could pass the whole input sequence through a recursive neural net, like we've seen before, and use the final hidden state as our first query. With that query, we can calculate our first output using the attention mechanism. We can use the translated output to update the state in our RNN, use the next hidden state as the next query, and so on, until we translated the whole sentence. Note that to realize this architecture, we'd also need special <Start> and <Stop> tokens, to tell the RNN when the translation starts and give it a way to mark that it wants to stop generating.

One cool thing that we can do with attention is taking a look at the attention weights. Shown here is an example from a paper on machine translation using an architecture quite similar to the one we just looked at, but translating to french. In the picture, you can see which english words the network paid attention to to generate each of the french outputs. What's really cool is that you can clearly see the network finding the correct words to attend to, even if they aren't the next one in the input. When translating 'the European Economic Area' to 'la zone économique européenne', the attention weights show the exact word inversion order.

Mechanisms like this that we can use to actually interpret how the network made its decisions are one of the key research areas of AI and AI Safety. But while in this example, explaining the attention weights works pretty well, it often falls short for large models. That is because to be able to interpret the reason why the model looked at specific neurons, we'd need to understand what these neurons encoded. And this often proves hard. Anthropic, the maker's of the LLM claude, state it like this:

"Unfortunately, the most natural computational unit of the neural network – the neuron itself – turns out not to be a natural unit for human understanding. This is because many neurons are polysemantic: they respond to mixtures of seemingly unrelated inputs."

For this reason, interpretation of attention in large models often falls short of its expectations. The results of interpretation in the AlphaFold paper mostly found out that in early layers, attention focused on residues that are next to each other in the sequence, and for late layers, attention focused on residues that are next to each other in the protein. Basically, this only restates that the network indeed found out the structure of the protein, but tells us little on the question of how.

But we aren't helpless! The explosion of large language models lead to a huge increase in research on interpretability, driven by the big ethical questions that accompany their massive use. The paper from Anthropic we talked about earlier for example found out a way to actually detangle that polysemanticity and clearly label activation patterns. This process might be transferrable to AlphaFold, possibly making us able to tell in which stages AlphaFold starts coming up with hypotheses on commonly known structure motives. 

This is one of the reasons why I think it's so important to not be too specific when learning AI. Many of the big breakthroughs came from taking a method that's commonly known in a specific AI area, like Natural Language Processing, and taking it to a new problem, like protein folding. It's why we spend so much time talking about language in this chapter, and it's why I highly recommend you to also take a look at Computer Vision at one point or another, for example with the open material from Justin Johnson's course from the University of Michigan.

Following Content:
Introduction of self-attention, noting that it is exactly identical to the ones before with the only difference that we are generating the queries via a linear layer directly from the inputs, meaning we have as many outputs as inputs, and that the value dimension if often chosen as the input dimension, so that the outputs and inputs have identical shape.

Gated self-attention with bias: Introduction of a bias (N, N) of the same shape as the dot-product affinities, that is added to them before normalization. In AlphaFold, that usually isn't a learned bias, but a (N, N) tensor that comes from a different source, to allow for new channels for information. For example, in the Evoformer AlphaFold has a pair representation of shape (N_res, N_res) that can represent information between individual amino acids, and uses this to add a bias that can be added to attention that's happening on the amino acid sequence of shape (N_res, c). Gating simply means constructing another gate embedding from the inputs using a linear layer and a sigmoid layer, giving values between 0 and 1. This is multiplied against the self-attention output to limit information pass-through for some of the outputs. 

Multi-Head Attention: Multi-Head attention is nothing but initializing multiple attention modules separately, passing them to each attention module and concatenating their outputs. The concatenated outputs are usually passed through another linear layer to get the desired number of output channels. However, we don't need to really construct this using separate modules. Instead, we can just do the embeddings jointly, creating queries and keys of shape (N, h, c) instead of (N, c). This is something important to notice: Doing two linear operations separately on the same input is identical to just using one linear layer with the added number of output channels, then splitting the outputs. You can think about how matrix multiplication works to see this. Concretely, a linear layer like shown here, going from c_i to (h, c) means taking a linear layer from c_i to h*c, then reshaping the output. 

Global Attention:
Attention has the big benefit that it works on sequences of data in a parallel fashion. This is super beneficial for computation speed, as we don't need to speed up the individual computations, but we can just use more compute devices parallely. The drawback is that attention has a really high memory usage, of N^2, for the intermediate affinities and attention weights. This is really problematic for large Ns. In AlphaFold's architecture, at one point, we have to handle a really high N, in the so called extra msa stack, where we take into account all sequences that are similar to the target sequence via sequence alignment. For this, AlphaFold uses a more light-weight attention approach with linear memory need: Global attention. For global attention, the query vectors are averaged out after computation. This means we have only one query vector, translating to only one attention weight per key and one output. However, AlphaFold uses global attention with the gating attention mechanism we've seen before. The gate values are calculated have the same shape as the input, and so the number of the outputs is restored in the broadcasted multiplication of the single attention output with the gate values. Still, the single attention output is a big bottleneck, but necessary to handle the large count of sequences.

Masked Self-Attention:
A typical task for attention networks (not in AlphaFold but in general) is next token prediction. This is for exmaple how large language models are trained: You give them a sequence of input tokens, and try to fit them to output the exactly same sequence, but shifted up. You can see the architecture shown here. But there's a catch: When we start with training the model, we have no problems to expect. We can even make an educated guess how the attention weights will look like: Something like this, where most of the attention is in the first upper diagonal of the matrix. That's because it's pretty helpful to know that the second word is love when trying to predict the one that comes after the first :D. If we try to use this trained model in inference however, to generate new text, we are facing a problem. When predicting the word after the Start token, the model will try really hard to get an attention update from the second token, only to be surprised that it doesn't exist yet. That's because the token needs to be predicted by the model itself before it can be used. 
Fortunately, there's an easy solution to this: It's to mask out the 'farther-in-future' keys, by setting the affinities to -infinity, which translates to an attention weight of 0 after softmax. This way, during training, they don't get any attention and are behaving exactly as if they weren't present at all, and we have identical behaviour as during inference, where we really feed in a shorter sequence since we don't know the upcoming words yet. Two things to note: We wouldn't have to solve this problem, if we didn't need to run the operations in parallel but it makes training a lot faster, second, during inference we will have to run this part of the model sequentially anyway, as we only get the query vectors one after another. 
This case of future masking with a stair pattern is one  of the two common uses of masking, the second is to mask out padded tokens that aren't really present in a sequence but padded to make all samples in the batch the same size. For this, we wouldn't need a stair pattern, but we could simply mask out the whole key column for this item in the batch.

Permutation: 
Next, we want to look at what happens when we change the order of the inputs in attention: 
Let's say we swap the second and third input. This means the query vectors swap as well, and so do the corresponding rows in the affinities. The keys and the corresponding columns do the same. This swap in the columns is mirrored by the columns in the normalized attention weights and the corresponding value vectors, as well as the rows in the raw attention scores. If we think about it, swapping columns in the attention scores and the value vectors doesn't change anything, since they are just multiplied and then summed up, disregarding the order, so we can swap the columns back. This means, all that changed here compared to the normal order is that two rows in the attention weights swapped, and after multiplication and summation with the value vectors, this means we have no change but a swap in the final outputs.
Changing the order of the inputs has no other impact but changing the order of the outputs. For this reason, self-attention is called a set-to-set operation, and it poses a problem: The order of elements in a sequence often has huge impact on the meaning, sentences aren't just a bag of words. Luckily, there's an easy solution, and it is to add positional encodings to the very first input in a model using attention. These positional embeddings can be either static functions of the index, like sinusoidal embeddings, or learned simply learned vectors. In the tutorial notebook for this video and in AlphaFold, we'll use learned positional embeddings. They are easier to implement and the quality of the results is about equal. Sinusoidal embeddings can sometimes generalize to inputs that are longer than the ones you trained the model on, in comparison to learned embeddings where you have to decide on a maximal number of sequence length.



[](https://transformer-circuits.pub/2023/monosemantic-features) [](https://www.anthropic.com/news/mapping-mind-language-model)



The two last additions are multi-head-attention and gated attention.
Hi everyone and welcome to this video on the Structure Module in AlphaFold. I'm Kilian Mandon and this video is the eighth, and second to last, in our series where we are implementing AlphaFold from scratch.

The Structure Module connects the output from the Evoformer to actual atom positions. We already completed most of what makes it difficult in the last video on geometry. Besides that, there are only two elements that are better explained, and we'll start right off with the first one: Invariant Point Attention. 

The Invariant point attention mechanism directly uses AlphaFold's current hypothesis on the position of the residues to calculate attention weights. A normal attention mechanism looks like this: The network predicts a set of key and query vectors, and they are interpreted as directions. We test how much they align by computing their dot-product, to calculate the attention scores. In AlphaFolds Invariant Point Attention module, these predicted query and key vectors are instead treated as actual points in 3-dimensional space. The query and key vectors of each residue are interpreted as positions relative to the residue location, and are globalized by applying the transform AlphaFold currently assigned to the residue in this iteration. Now, instead of calculating the alignment of the vectors, AlphaFold calculates their distance to compute the attention weight: query-key pairs that are close to each other get high attention and pairs that are far from each other get low attention. 

In addition to using the residue transforms for calculating the attention weights, they are also used to compute the attention values. In Invariant Point Attention, the values for each key are created as 3D points relative to the coordinate frame the key belongs to. The way in which they affect the output for the queries is as the coordinates of the value point in the coordinate system of the query frame. 

You can imagine that an attention mechanism that assigns attention based on closeness in 3D space can be really helpful for calculating updates to the positions. The query and key positions could be predicted as positions that are highly relevant for the chemical interactions of the individual residues. Standard attention doesnt really have a mechanism to take this into account: The queries and keys are predicted using nothing but the embedding of the residue. However, we probably shouldn't be that hard on regular attention, given that we can see in the attention weights in the evoformer that the model already assigns attention scores that strongly match closeness in the final predicted structure. Still, IPA is a clever concept and it worked arguably well in AlphaFold.

So, let's see how the mechanism works in code. The first thing to spot is that the point-attention isn't the only contribution to the values and the attention weights. In line 1 we calculate query, key and value vectors just the way we used to. The local coordinates of the query, key and value points are predicted in line 2 and 3. In line 4 we compute a bias for the attention weights from the pair representation, similar to how we did in the Evoformer, and line 5 and 6 compute scaling factors to even out the contribution of the different factors influencing attention - the bias, the normal query-key similarity and the query and key points. 

Then, in line 7, the attention weights are calculated. The first two terms - dot-product similarity and bias - are the same as always. For the third term, the query and key points are promoted to global coordinates by the respective residue transforms. Note that the final residue transforms aren't known at this point, but AlphaFold runs multiple iterations of the Structure Module, and in each iteration, invariant point attention uses the currently predicted backbone transforms. The distance of the global points is subtracted from the attention weights, scaled by a factor for weighting. One part of the weight, gamma, is actually not constant but a learnable factor.

After the attention weights are calculated, AlphaFold creates three different attention outputs from the attention weights. The first one is produced by multiplying the attention weights with the pair representation pointwise, then summing the values up along the key dimension. The second is the typical attention output, multiplying the attention weights with the corresponding value vectors, then summing them up. And the third uses 3D points as values. Concretely, the predicted 3D value points are globalized by the transform of the key they are associated with, followed by computing the coordinates of this point with respect to the transform of the query they are calculated for, before summing them up. All of the different attention outputs are concatenated and fed through a linear layer to get them to the desired output shape.

The weighting factors for the different terms are chosen so that all values contribute about equally. Specifically, the three parts—dot-product affinity, bias, and point attention—are scaled to have a variance of 1. This scaling assumes that all network outputs are independently Gaussian distributed, which is a common approach when designing layer activations.

However, point attention presents a unique challenge, as it’s difficult to account for the transforms of the amino acids and quantify their influence on the attention value. To address this complexity, or perhaps to provide the model with more flexibility, AlphaFold introduces a learnable parameter, gamma, to scale the point attention. Gamma is computed as the softplus—a smooth, static function similar to ReLU—of a learned parameter, which is initially set to zero.

Now that we understood invariant point attention, let's take a look at the Structure Module. Looking at the code, most of it are just standard linear and normalization layers. For the general structure, two things are of note: First, the Structure Module runs 8 times in a row, refining the backbone transforms in each iteration. Each of the iterations uses the same weights, meaning that we don't initialize multiple blocks for this approach. The initial transforms are set to identity rotation and translation 0, meaning all amino acids start at the origin in the first iteration. This is called a "Black hole initialization" in the paper.

Each iteration of the structure module consists of the following blocks: First comes the invariant point attention module we just checked out. Then, in line 8, comes a short feed-forward module, the transition. After that, the network computes a transform via the module BackboneUpdate, and uses it to modify the current transforms by right-composition. We discussed in the last video that right-composition is generally favorable for updates that cause less leverage effects. After the Backbone Update, the structure module computes the torsion angles using a short feed-forward network with residual connections, the so called angle resnet. 

During inference, it's not really necessary that we compute the angles in every iteration of the Structure Module, as only the last result is used for the output. The code is structure like this because the loss of the network, which is omitted here, actually uses the predictions of each iteration. It's not only trained to give a good prediction on the last go, but to already give good outputs in every iteration.
The prediction of the transforms in each iteration *is* necessary during inference: Both for invariant point attention in the next iteration, and because its iteratively updated. After the Structure module ran through, the final predictions of the transforms and torsion angles are used in the method computeAllAtomCoordinates, which we implemented in the last video, to calculate the final atom positions.

The feed-forward parts of the structure module are straight-forward to implement, and we already covered InvariantPointAttention. What's left is the backbone update. We took a quick look at the code in the last video already. Basically, the network predicts a quaternion and a translation for each residue with a linear layer. The quaternion is normalized to norm 1 and converted to a rotation matrix using the conversion we discussed in the last video - applying the quaternion to the unit vectors 100, 010 and 001, then stacking those as columns in the matrix.

One thing of note here is that the network doesn't predict four values here, but only three. The quaternion is constructed by padding with a 1 as the scalar art, then normalizing it. This might be a bit surprising, given that in the last video, we elaborated that for a uniform rotation, we predict four gaussian values, then scale these to 0. The reason why AlphaFold doesn't do it like this comes from the iterative nature of the Structure Module. If we had a single shot prediction, we'd directly want a uniform rotation. But, taking into account that AlphaFold iteratively updates the prediction in the Structure Module, it's better to have smaller rotations. This is done by using a scalar one as w. By fixating this value, the resulting quaternion is closer to the identity quaternion 1000, resulting in a smaller rotation. Note that the direction of the axis of rotation is only dependent on the vector part of the quaternion, so it's still spherically uniform. 

With this, we have all the modules that we need for the Structure Module. It's the last part of AlphaFold, so it's also responsible for gathering all the outputs we need. Concretely, it returns five values: The torsion angles, the backbone transforms, the final positions of all heavy atoms, the mask which atoms are actually present in the amino acids, and the pseudo-beta positions. The pseudo-beta positions are used by the recycling embedder for the next iteration of AlphaFold, and they are the positions of the beta-c atom except for the amino acid glycine, which lacks a sidechain. For glycine, its the alpha-c atom. 
One fact to pay attention to is that, in the model, AlphaFold uses nanometers as the distance unit. This is mostly relevant for the calculation in the invariant point attention module, and to fit the output to an order of magnitude that's suitable for neural networks. In the recycling embedder and for the final atom positions, we use angstrom instead, which is larger than nanometers by a factor of 10. To do so, the translation vectors of the transforms need to be scaled by 10 - nothing else needs to change, since the rotations aren't affected by distance.

With that, we are done with the Structure Module. This was a relatively short video, because we did most of the hard work in the previous one already. This holds true for the next video as well, where we put all of the parts together. In the accompanying jupyter notebook, you can do the full implementation of the Structure Module yourself. So, see you in the next video and happy coding!
Hi everyone and welcome to this video on feature embedding in AlphaFold! I'm Kilian Mandon and this video is the sixth in our series where we'll be implementing AlphaFold from scratch.

With feature embedding, we're bridging the gap between feature extraction and the Evoformer, which we've built in the last videos. Feature embedding refers to the first few layers of a network, where we embed the initial features - which are often one-hot encoded - into the first, learned embedding vectors. Additionally, we are creating positional encodings for the model, to make up for the fact that the attention mechanism has no grasp of order without them, and we'll build the Recycling Embedder that uses the predicted output of the model to feed it into a new round, enhancing the prediction step-by-step.

So, let's look into how all of that works in a little more detail:

...
In the second to last video, with feature extraction, we constructed the four tensors on the left: The extra MSA feature, the target feature, the MSA feature, and the residue_index, which is just a range of numbers from 0 to r-1. In the last video, we implemented the Evoformer, as shown on the right of the diagram. In between, we have three different components: First the input embedder, which constructs the pair representation and the MSA representation from the features. Then, the Extra MSA Stack, which updates the pair representation using the extra MSA features, that is the sequences not selected as cluster centers during feature extraction. And last the Recycling Embedder: The entire AlphaFold model is run multiple times for a single protein, and the Recycling Embedder updates the pair and MSA representations using the structure AlphaFold predicted in the previous iteration.

So, let's start with the input embedder. It's a relatively simple architecture, and the diagram translates to code almost directly. The target feature is embedded by two separate linear layers, and we compute their outer sum to get it to the shape of the pair representation. The positional encoding is created by a method called relpos, which we'll look at shortly, and added to the initial pair representation. For the construction of the MSA representation, both the MSA feature and target feature are embedded by a linear layer and added up. Note that The target feature needs to be broadcasted to match the shape of the MSA representation, which is written as "tile" in the flowchart. 

The creation of the relative position encodings is straightforward as well. The first part of the algorithm is calculating d as the outer difference of the residue index with itself. The resulting matrix looks like this. d is then one_hot encoded. As indicated by the yellow highlight, one_hot is a specific algorithm described in the paper and its code is as follows. It calculates the index of the element in the bins x is closest to, and sets this value in the result to one, while leaving all others as zero. However, this is a bit of an overkill here. The pairwise distances are integers, and we can use them directly as class indices. All we need to do is add 32 to shift the range to start at 0, and then we can one-hot encode them using the standard PyTorch method.

So, with the relative position encodings finished, the input embedder is ready and we can move on to the Recycling Embedder.

During inference, AlphaFold uses a concept they call recycling, which means that the full model is run multiple times, and the outputs of the last iteration are fed into the new iteration, setting them to zero in the first iteration where there are no previous outputs to use. 

To train using this concept, AlphaFold randomly samples a number of recycling iterations between 1 and the maximum number of cycles intended for use during inference. However, the parameters are only optimized to get the last iteration right. That means their contribution to the final output through the previous iterations, which is carried on through the Recycling Embedder, isn't considered in the parameter update. This is done by the stopgrad command which stops the gradient flow at this point during backpropagation. In their paper, the developers calculate that this version of randomly sampling the number of recycling iterations and only doing backpropagation through the last iteration increases the training time by 37.5% compared to a single iteration, while a full unrolling of the forward pass and a fixed number of four recycling iterations would increase training time by 300%.

The way in which the last outputs are included as model inputs is by the Recycling Embedder. It uses three of the model's outputs: The first row of the MSA representation and the pair representation, which are outputs of the Evoformer in the last iteration, and the predicted pseudo-beta carbon positions from the structure module. 

<!-- Hier noch ein Bild rein -->
The beta carbon atom is the first side-chain carbon for amino acids. Here, it is called pseudo-beta because for the one amino acid that doesn't have a side-chain carbon, glycine, the C-alpha carbon is used instead. 

With this in mind, the recycling embedder is relatively straightforward. It calculates the pairwise distances between the beta-carbons, assigns them to a number of bins, and performs one-hot encoding followed by a linear layer on them. The pair representation and the first line of the MSA representation are normalized using layer normalization and the embedded pairwise beta-carbon distances are added to the pair representation.

One thing to note here is that the exact values for the bins in our implementation will differ from this pseudocode. This is because we have to follow the exact implementation in OpenFold to make our model compatible with their weights. 
In our implementation, our bins are drawn up as 15 values linearly spaced between 3.25A and 20.75A, and a value lies in a bin if it's on the right side of these borders. 
In code, this logic looks like this: We create a list of lower bounds for the bins and create the upper bounds as the shifted lower bounds, with infinity as the last upper bound. To assign the values to the bins, we simply check if each value is greater than the lower bound and less than the upper bound for each bin. We can do that with a broadcasted "less than" and "greater than" and by using multiplication for the logical "and". This way, we directly end up with a one-hot encoding of the distances. Note that with this implementation, distances less than 3.25A will end up as a full zero vector, which translates to them being just the bias vector of the linear layer they are passed through.
<!-- on from here -->
With that, the Recycling Embedder is ready and we can move on to the last part of feature embedding: The Extra MSA Stack. After a linear embedding of the extra MSA feature, it works on the extra MSA representation and the pair representation. It's the reason why we implemented the Evoformer before doing feature embedding, because it's almost identical. This is the pseudocode for the Extra MSA Stack, and everything that's different from the Evoformer is highlighted in yellow. During MSA row attention, the extra MSA stack uses an embedding dimension of 8 instead of 32, and instead of normal column attention, it uses global attention. Also note that we're only using four blocks instead of 48. Besides that, everything is identical: In the MSA stack we have row-wise attention with pair bias for communication with the pair stack, column attention, and the two-layer feed-forward MSA transition, outer product mean to get the extra MSA representation to the shape of the pair representation, and then in the pair stack outgoing triangle multiplication, incoming triangle multiplication, triangle attention around the starting node and triangle attention around the ending node. 
<!-- Show images of outoging and incoming here -->
As a quick reminder: Outgoing triangle multiplication calculates the entry ij as the sum-product between the rows i and j. Incoming triangle multiplication calculates the entry ij as the sum-product between the columns i and j. Attention around the starting node is row-wise attention with the pair representation itself as bias, and attention around the ending node is column-wise attention with the transposed pair representation as bias. 

As we discussed in the video on attention, global attention is a way to reduce the memory footprint for attention with a too-high sequence length. This is the case here because of the large number of extra sequences, which is the attention dimension for column-wise attention on the extra MSA representation. For global attention, the queries are averaged out to only one query. This leads to only one row of attention weights and only one output. The dimension is restored when broadcasting the attention output against the gate values. We didn't look at the pseudocode for global attention before in the video attention, so we can catch up here: Highlighted in yellow are all the differences to normal attention. In line 2, you can see that we're only using one set of keys and values for the different attention heads. In line 3, you can see how the queries are averaged out. As a result, the attention weights "a" have only two indices t and i, t for the different keys and i for the different columns, as attention is calculated for each column individually. For normal attention, the indices of a would be a_{sti}.
Last, in line 6, we can see how the row dimension is restored by multiplication with the gates g, as the row index s is only present in the gates and therefore broadcasted against the weighted value vectors. However, if you already finished the notebook on attention, you don't need to write any of this because we already did it. All you need to do is set the is_global flag to true.
...

With that, we are done with feature embedding. It's a relatively small part of AlphaFold, and you'll probably need a little less time for actually writing the code as well. As always, you can find a jupyter notebook in the description where you can do the full implementation yourself. Basically, all that's left to do now is building the structure module and stitching all the parts together. But, building the structure module requires some non-standard knowledge on 3D geometry: We'll use rotation matrices, homogenous coordinates and quaternions to build the three dimensional structure of the protein. So, if all of these are words you never heard before, or if it's something that you heard but never truly understood, I'll be more than happy to explain them in the next video - I'll think it'll be my favorite in the series.
So, see you there and happy coding!
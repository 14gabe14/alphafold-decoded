{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "\n",
    "Run the cells below for the basic setup of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print('No colab environment, assuming local setup.')\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "    # turorials folder, e.g. 'alphafold-decoded/tutorials'\n",
    "    FOLDERNAME = None\n",
    "    assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "    # Now that we've mounted your Drive, this ensures that\n",
    "    # the Python interpreter of the Colab VM can load\n",
    "    # python files from within it.\n",
    "    import sys\n",
    "    sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "    %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "\n",
    "    print('Connected COLAB to Google Drive.')\n",
    "\n",
    "import os\n",
    "    \n",
    "base_folder = 'tensor_introduction'\n",
    "control_folder = f'{base_folder}/control_values'\n",
    "\n",
    "assert os.path.isdir(control_folder), 'Folder \"control_values\" not found, make sure that FOLDERNAME is set correctly.' if IN_COLAB else 'Folder \"control_values\" not found, make sure that your root folder is set correctly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to change this path, depending on your directory structure and current working directory\n",
    "control_folder = 'attention/control_values'\n",
    "assert os.path.isdir(control_folder), 'Folder for control values not found. Please check your path.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "Attention is the underlying mechanism for most of the biggest breakthroughs in Machine Learning in the last years. Google published the original transformer paper under the name 'Attention Is All You Need' and so far, it lived up to its expectation.\n",
    "\n",
    "In this Notebook, we will implement the following attention mechanisms:\n",
    "\n",
    "- MultiHeadAttention\n",
    "- Gated MultiHeadAttention\n",
    "- Global Gated MultiHeadAttention\n",
    "\n",
    "These modules will do the heavy lifting for the Evoformer, the first part of AlphaFold's architecture. The rest of the Evoformer will mostly be about stacking the layers correctly. All of them will be implemented in the class `MultiHeadAttention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, head over to `mha.py` and implement the `__init__` method and `prepare_qkv`. Don't worry about the global parameter for now, treat as if it were set to False. `prepare_qkv` will rearrange the tensors, so that the different heads are split up and  the attention dimension is moved to a fixed position.\n",
    "\n",
    "Run the following code cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention.mha import MultiHeadAttention\n",
    "\n",
    "c_in = 8\n",
    "c = 10\n",
    "N_head = 4\n",
    "attn_dim = -3\n",
    "\n",
    "mha = MultiHeadAttention(c_in, c, N_head, attn_dim=attn_dim, gated=True)\n",
    "\n",
    "param_shapes = { name: param.shape for name, param in mha.named_parameters()}\n",
    "\n",
    "expected_shapes = {\n",
    "    'linear_q.weight': (40, 8),\n",
    "    'linear_k.weight': (40, 8), \n",
    "    'linear_v.weight': (40, 8), \n",
    "    'linear_o.weight': (8, 40), \n",
    "    'linear_o.bias': (8,),\n",
    "    'linear_g.weight': (40, 8), \n",
    "    'linear_g.bias': (40,),\n",
    "}\n",
    "\n",
    "assert param_shapes.keys() == expected_shapes.keys()\n",
    "assert param_shapes.items() == expected_shapes.items()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_shape = (3, 5, 4, 8) + (N_head*c,)\n",
    "q = torch.linspace(-4, 4, steps=math.prod(qkv_shape)).reshape(qkv_shape)\n",
    "k = torch.linspace(-3, 3, steps=math.prod(qkv_shape)).reshape(qkv_shape)\n",
    "v = torch.linspace(-2, 2, steps=math.prod(qkv_shape)).reshape(qkv_shape)\n",
    "\n",
    "q_prep, k_prep, v_prep = mha.prepare_qkv(q, k, v)\n",
    "\n",
    "expected_q = torch.load(f'{control_folder}/prepped_q_local.pt')\n",
    "expected_k = torch.load(f'{control_folder}/prepped_k_local.pt')\n",
    "expected_v = torch.load(f'{control_folder}/prepped_v_local.pt')\n",
    "\n",
    "assert torch.allclose(q_prep, expected_q)\n",
    "assert torch.allclose(k_prep, expected_k)\n",
    "assert torch.allclose(v_prep, expected_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the forward pass through the MultiHeadAttention module. Again, don't worry about global attention for now. The method contains step-by-step instructions for the implementation.\n",
    "\n",
    "After you're done, check your implementation with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in = 8\n",
    "c = 10\n",
    "N_head = 4\n",
    "attn_dim = -3\n",
    "query_dim = 4\n",
    "\n",
    "inp_shape = (3, 5, query_dim, 8, c_in)\n",
    "inp = torch.linspace(-4, 4, math.prod(inp_shape)).reshape(inp_shape)\n",
    "\n",
    "# Check for ungated attention\n",
    "\n",
    "mha_ungated = MultiHeadAttention(c_in, c, N_head, attn_dim=attn_dim, gated=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for key, param in mha_ungated.named_parameters():\n",
    "        param.copy_(torch.linspace(-4, 4, param.numel(), device=param.device).reshape(param.shape))\n",
    "\n",
    "    out_ungated = mha_ungated(inp)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/forward_nogate_nonglobal.pt')\n",
    "\n",
    "assert torch.allclose(out_ungated, expected_result)\n",
    "\n",
    "# Check for gated attention\n",
    "\n",
    "mha_gated = MultiHeadAttention(c_in, c, N_head, attn_dim=attn_dim, gated=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for key, param in mha_gated.named_parameters():\n",
    "        param.copy_(torch.linspace(-4, 4, param.numel(), device=param.device).reshape(param.shape))\n",
    "\n",
    "    out_gated = mha_gated(inp)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/forward_gated_nonglobal.pt')\n",
    "\n",
    "assert torch.allclose(out_gated, expected_result)\n",
    "\n",
    "# Check for gated attention with bias\n",
    "\n",
    "bias = torch.linspace(-1, 1, N_head*query_dim**2).reshape(N_head, query_dim, query_dim)\n",
    "with torch.no_grad():\n",
    "    out_with_bias = mha_gated(inp, bias=bias)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/forward_gated_bias_nonglobal.pt')\n",
    "\n",
    "assert torch.allclose(out_with_bias, expected_result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we will implement the global self-attention mechanism. It will be used in the ExtraMSA stack in AlphaFold to account for the large number of sequences. \n",
    "\n",
    "Global self-attention has two major differences:\n",
    "- For the key and value embeddings, only one head is used\n",
    "- The query vectors will be averaged over the query dimension, so that only one query vector will be used for the attention mechanism\n",
    "\n",
    "Thinking back to the attention mechanism, the number of query vectors determines the number of outputs of the layer, so the global attention variant would reduce the number of outputs. However, AlphaFold only uses gated global attention, and the number of outputs is restored when broadcasting the weighted value vectors against the gate embedding.\n",
    "\n",
    "Implement the method `prepare_qkv_global`. Also, modify the `__init__` method so that key and value embeddings use only one head when is_global is set, and modify the `forward` method so that `prepare_qkv_global` is called instead of `prepare_qkv` if is_global is set. You won't have to do any other modifications to forward, but it might be helpful to carefully look through the function and see why that's the case.\n",
    "\n",
    "Test your code with the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in = 8\n",
    "c = 10\n",
    "N_head = 4\n",
    "attn_dim = -3\n",
    "query_dim = 4\n",
    "\n",
    "inp_shape = (3, 5, query_dim, 8, c_in)\n",
    "inp = torch.linspace(-4, 4, math.prod(inp_shape)).reshape(inp_shape)\n",
    "\n",
    "mha_global = MultiHeadAttention(c_in, c, N_head, attn_dim=attn_dim, gated=True, is_global=True)\n",
    "\n",
    "# Test for prepare_qkv_global\n",
    "\n",
    "qkv_shape = (3, 5, 4, 8) + (N_head*c,)\n",
    "q = torch.linspace(-4, 4, steps=math.prod(qkv_shape)).reshape(qkv_shape)\n",
    "k = torch.linspace(-3, 3, steps=math.prod(qkv_shape)).reshape(qkv_shape)\n",
    "v = torch.linspace(-2, 2, steps=math.prod(qkv_shape)).reshape(qkv_shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_prep, k_prep, v_prep = mha_global.prepare_qkv_global(q, k, v)\n",
    "\n",
    "\n",
    "expected_q = torch.load(f'{control_folder}/prepped_q_global.pt')\n",
    "expected_k = torch.load(f'{control_folder}/prepped_k_global.pt')\n",
    "expected_v = torch.load(f'{control_folder}/prepped_v_global.pt')\n",
    "\n",
    "assert torch.allclose(q_prep, expected_q)\n",
    "assert torch.allclose(k_prep, expected_k)\n",
    "assert torch.allclose(v_prep, expected_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for key, param in mha_global.named_parameters():\n",
    "        param.copy_(torch.linspace(-4, 4, param.numel(), device=param.device).reshape(param.shape))\n",
    "\n",
    "    out = mha_global(inp)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/forward_global.pt')\n",
    "\n",
    "assert torch.allclose(expected_result, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With this chapter, we are done with the introductory material. In the next chapter, we will implement the input feature extractor, the module that builds the numeric input tensors for the model from the raw MSA text file.\n",
    "\n",
    "If you want to learn more about attention, you can check out the later assignments from CS231n (the Computer Vision course from Stanford we suggested in the last chapter) or the [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/), an online Jupyter Notebook that explains the Transformer Architecture, which powers modern LLMs like ChatGPT."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

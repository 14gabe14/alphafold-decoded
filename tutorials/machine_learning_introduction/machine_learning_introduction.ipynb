{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from solutions.machine_learning_introduction.feed_forward import TwoLayerNet, train_model\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatible GPU available, using CUDA.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Compatible GPU available, using CUDA.')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('No compatible GPU available. Using CPU.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to change this path, depending on your directory structure and current working directory\n",
    "control_folder = 'solutions/machine_learning_introduction/control_values'\n",
    "assert os.path.isdir(control_folder), 'Folder for control values not found. Please check your path.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "In this section, we wilil implement a simple two-layer fully-connected Neural Network and apply it to the MNIST digit classification task we performed before in the tensor introduction.\n",
    "\n",
    "For that, we will first implement some layers (affine linear, sigmoid and relu) and then assemble those layers into a classifier. \n",
    "\n",
    "The code for the layers will be structured into two methods: a `forward` and a `backward` method. The `forward` method will receive the input tensor and the layer's parameters and will return the output and a cache of all the values necessary for backpropagation. The `backward` method will receive the upstream gradient and the cache from the forward pass and will compute the downstream gradient and the gradients of the layer's parameters. The functions will be in the following format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_forward(x, W):\n",
    "    # Calculation of out depends on the layer formulation\n",
    "    out = ...\n",
    "    # Necessary values depend on the layer formulation\n",
    "    cache = (x, out)\n",
    "    return out, cache\n",
    "\n",
    "def layer_backward(dout, cache):\n",
    "    x, out = cache\n",
    "    dx = ...\n",
    "    # Gradients of the parameters, if the layer has any\n",
    "    dW = ...\n",
    "    return dx, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with `affine_forward` and `affine_backward`. Head over to `feed_forward.py` and implement the forward and backward pass. Use the following notebook cells to validate your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import affine_forward\n",
    "\n",
    "# Check for affine_forward\n",
    "\n",
    "N = 3\n",
    "c_in = 16\n",
    "c_out = 10\n",
    "\n",
    "x = torch.linspace(-4, 4, N*c_in).reshape((N, c_in))\n",
    "W = torch.linspace(-4, 4, c_in*c_out).reshape((c_out, c_in))\n",
    "b = torch.linspace(-4, 4, c_out)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, cache = affine_forward(x, W, b)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/affine_forward_out.pt')\n",
    "\n",
    "assert torch.allclose(out, expected_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import affine_backward\n",
    "\n",
    "# Check for affine_backward\n",
    "\n",
    "N = 3\n",
    "c_in = 16\n",
    "c_out = 10\n",
    "\n",
    "dout = torch.linspace(-4, 4, N*c_out).reshape(N, c_out)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dx, dW, db = affine_backward(dout, cache)\n",
    "\n",
    "expected_dx = torch.load(f'{control_folder}/affine_backward_dx.pt')\n",
    "expected_dW = torch.load(f'{control_folder}/affine_backward_dW.pt')\n",
    "expected_db = torch.load(f'{control_folder}/affine_backward_db.pt')\n",
    "\n",
    "assert torch.allclose(dx, expected_dx)\n",
    "assert torch.allclose(dW, expected_dW)\n",
    "assert torch.allclose(db, expected_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement `relu_forward` and `relu_backward`. We will use ReLU as the activation function for the hidden layer. Run the following code cells to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import relu_forward\n",
    "\n",
    "# Check for relu_forward\n",
    "\n",
    "N = 5\n",
    "c = 10\n",
    "x = torch.linspace(-4, 4, N*c).reshape((N, c))\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, cache = relu_forward(x)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/relu_forward_out.pt')\n",
    "\n",
    "assert torch.allclose(expected_result, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import relu_backward\n",
    "\n",
    "# Check for relu_backward\n",
    "\n",
    "N = 5\n",
    "c = 10\n",
    "dout = torch.linspace(-4, 4, N*c).reshape((N, c))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dx = relu_backward(dout, cache)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/relu_backward_dx.pt')\n",
    "\n",
    "\n",
    "assert torch.allclose(expected_result, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer we will implement is the sigmoid activation. We will use it to normalize the output of our module to the range [0, 1]. Implement the methods `sigmoid_forward` and `sigmoid_backward`. Use the following code cells to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import sigmoid_forward\n",
    "\n",
    "# Check for sigmoid_forward\n",
    "\n",
    "N = 5\n",
    "c = 10\n",
    "x = torch.linspace(-4, 4, N*c).reshape((N, c))\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, cache = sigmoid_forward(x)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/sigmoid_forward_out.pt')\n",
    "\n",
    "assert torch.allclose(expected_result, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import sigmoid_backward\n",
    "\n",
    "# Check for sigmoid_backward\n",
    "\n",
    "N = 5\n",
    "c = 10\n",
    "dout = torch.linspace(-4, 4, N*c).reshape((N, c))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dx = sigmoid_backward(dout, cache)\n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/sigmoid_backward_dx.pt')\n",
    "\n",
    "\n",
    "assert torch.allclose(expected_result, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Accuracy\n",
    "\n",
    "We will use L2-Loss for our model. This is not the most typical loss for classification (SVM-Loss or Softmax-Loss are more commonly used), but it is easy to understand and works well enough for this simple task. The output of our model will be of shape (N, num_classes). Each entry represents the certainty for this digit. The optimal output for an image labeled as a 2 would then be (omitting the batch dimension):\n",
    "\n",
    "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] \n",
    "\n",
    "This is called one-hot encoding. You can use `nn.functional.one_hot` for the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head over to `feed_forward.py` and implement `l2_loss`. Check your implementation with the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import l2_loss\n",
    "\n",
    "# Check for l2_loss\n",
    "\n",
    "N = 5\n",
    "num_classes = 12\n",
    "y_pred = torch.load(f'{control_folder}/l2_loss_y_pred.pt')\n",
    "y_label = torch.load(f'{control_folder}/l2_loss_y_label.pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss, grad = l2_loss(y_pred, y_label)\n",
    "\n",
    "expected_loss = torch.load(f'{control_folder}/l2_loss_loss_value.pt')\n",
    "expected_grad = torch.load(f'{control_folder}/l2_loss_grad.pt')\n",
    "\n",
    "assert loss.shape == expected_loss.shape\n",
    "assert grad.shape == expected_grad.shape\n",
    "assert torch.allclose(loss, expected_loss)\n",
    "assert torch.allclose(grad, expected_grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our last method before going on to the classifier itself, we will implement `calculate_accuracy`. It will be used to calculate the training and validation accuracy during model training. Check your code with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import calculate_accuracy\n",
    "\n",
    "N = 20\n",
    "num_classes = 4\n",
    "inp = torch.zeros((0,))\n",
    "out = torch.load(f'{control_folder}/calculate_accuracy_out.pt')\n",
    "labels = torch.load(f'{control_folder}/calculate_accuracy_labels.pt')\n",
    "\n",
    "class _DummyModel:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return out, None\n",
    "model = _DummyModel(num_classes)\n",
    "\n",
    "acc = calculate_accuracy(model, inp, labels)\n",
    "\n",
    "assert torch.allclose(acc, torch.tensor([0.2]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "For the classification, we will build a two-layer feed-forward neural network. The architecture will be the following:\n",
    "\n",
    "(N, d) - Affine - (N, d_hidden) - ReLu - Affine - (N, num_classes) - Sigmoid\n",
    "\n",
    "The bracketed expressions note the shape of the inputs and outputs to the affine layers. The last layer is a sigmoid layer, so that the outputs are normalized between 0 and 1.\n",
    "\n",
    "The two affine layers require parameters (weights and biases). Implement the `__init__` method in the class TwoLayerNet and initialize the weights and biases as following:\n",
    "- Initialize both biases with zeros.\n",
    "- Initialize the weight of the first layer normally distributed with std sqrt(2 / c_in)\n",
    "- Initialize the weight of the second layer normally distributed with std sqrt(2 / (c_in + c_out))\n",
    "\n",
    "where c_in and c_out are the input and output dimensions of the layers respectively.\n",
    "\n",
    "The first variant is called He initialization and is mostly used for layers with ReLU activation, the second is called Xavier Normal and is primarly used for sigmoid and tanh activations. This choice of initialization tries to keep the variance of the output similar to the variance of the input. Proper initializations are crucial to avoid gradient explosion or gradient vanishing. This is particularly important for training deeper networks, and the effect can be mitigated by introducing batch normalization or layer normalization.\n",
    "\n",
    "However, we don't want to focus too much on initialization right. Implement the `__init__` method using the values above and test your code with the following cell. Name your weights 'W1', 'b1', 'W2', and 'b2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.machine_learning_introduction.feed_forward import TwoLayerNet\n",
    "\n",
    "inp_dim = 100\n",
    "hidden_dim = 50\n",
    "out_dim = 40\n",
    "\n",
    "model = TwoLayerNet(inp_dim, hidden_dim, out_dim)\n",
    "\n",
    "assert set(model.params.keys()) == {'W1', 'b1', 'W2', 'b2'}\n",
    "\n",
    "for key, param in model.params.items():\n",
    "    if key == 'W1':\n",
    "        correct_std = 0.1414213562373095\n",
    "        assert abs(torch.std(param).item() - correct_std) / correct_std < 7e-2\n",
    "    elif key == 'W2':\n",
    "        correct_std = 0.14907119849998599\n",
    "        assert abs(torch.std(param).item() - correct_std) / correct_std < 7e-2\n",
    "    elif 'b' in key:\n",
    "        assert torch.allclose(param, torch.zeros_like(param))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the forward pass of the model. Like the individual layers, it should return the output and a cache object, which will just be a tuple of the caches of all the layers. Check your implementation with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_params = {key: param.clone() for key, param in model.params.items()}\n",
    "\n",
    "N = 5\n",
    "inp_dim = 100\n",
    "hidden_dim = 50\n",
    "out_dim = 40\n",
    "\n",
    "inp = torch.linspace(-2, 2, inp_dim*N).reshape((N, inp_dim))\n",
    "\n",
    "for key, param in model.params.items():\n",
    "    model.params[key] = torch.linspace(-2, 2, torch.numel(param)).reshape(param.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, _ = model.forward(inp)\n",
    "\n",
    "for key in model.params.keys():\n",
    "    model.params[key] = old_params[key]\n",
    "    \n",
    "\n",
    "expected_result = torch.load(f'{control_folder}/model_forward_out.pt')\n",
    "\n",
    "assert torch.allclose(out, expected_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the backward pass. This will involve calling the backward passes of the individual layers in reverse order. Check your code with the following cell. Save the gradients of the parameters in the `grads` dictionary. Make sure you use the same keys you used for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_params = {key: param.clone() for key, param in model.params.items()}\n",
    "\n",
    "N = 5\n",
    "inp_dim = 100\n",
    "hidden_dim = 50\n",
    "out_dim = 40\n",
    "\n",
    "inp = torch.linspace(-2, 2, inp_dim*N).reshape((N, inp_dim))\n",
    "dout = torch.linspace(-2, 2, out_dim*N).reshape((N, out_dim))\n",
    "\n",
    "for key, param in model.params.items():\n",
    "    model.params[key] = torch.linspace(-2, 2, torch.numel(param)).reshape(param.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, cache = model.forward(inp)\n",
    "    model.backward(dout, cache)\n",
    "\n",
    "expected_grads = torch.load(f'{control_folder}/model_backward_grads.pt')\n",
    "\n",
    "for key, grad in model.grads.items():\n",
    "    assert torch.allclose(grad, expected_grads[key]), f'Gradient for parameter {key} incorrect.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier\n",
    "\n",
    "We will implement a simple training loop and fit our model to the MNIST dataset of handwritten digits. First, we will load the dataset in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load and Prepare MNIST Dataset ---\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])  \n",
    "\n",
    "# Download and split into training and validation sets\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "validation_set = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Load all training and validation images and labels\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set))\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=len(validation_set))\n",
    "\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "validation_images, validation_labels = next(iter(validation_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of considering the image as a two-dimensional entity, we will flatten it so that it is one large feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# TODO: Flatten the train and validation images.                         #\n",
    "##########################################################################\n",
    "\n",
    "# Replace \"pass\" statement with your code\n",
    "pass\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################\n",
    "\n",
    "assert train_images.shape == (60000, 784)\n",
    "assert validation_images.shape == (10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a TwoLayerNet with input and output dimensions fitting to our task. You can choose the hidden dimension, 64 might be a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "##########################################################################\n",
    "# TODO: Initialize the TwoLayerNet by choosing the dimensions correctly. #\n",
    "##########################################################################\n",
    "\n",
    "# Replace \"pass\" statement with your code\n",
    "pass\n",
    "\n",
    "##########################################################################\n",
    "#               END OF YOUR CODE                                         #\n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code cell to transfer the data and parameters to a GPU for training, if you have a compatible GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "validation_images = validation_images.to(device)\n",
    "validation_labels = validation_labels.to(device)\n",
    "\n",
    "for key, param in model.params.items():\n",
    "    model.params[key] = param.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the method `train_model`. Instructions for the implementation are placed in the TODO message in the method.\n",
    "Run the following code cell to execute your training. Depending on your chosen hidden_dim, batch_size and learning_rate, you can expect accuracies above 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with epoch 0...\n",
      "    Train accuracy: 0.91\n",
      "    Validation accuracy: 0.92\n",
      "\n",
      "Starting with epoch 1...\n",
      "    Train accuracy: 0.93\n",
      "    Validation accuracy: 0.93\n",
      "\n",
      "Starting with epoch 2...\n",
      "    Train accuracy: 0.94\n",
      "    Validation accuracy: 0.94\n",
      "\n",
      "Starting with epoch 3...\n",
      "    Train accuracy: 0.95\n",
      "    Validation accuracy: 0.95\n",
      "\n",
      "Starting with epoch 4...\n",
      "    Train accuracy: 0.95\n",
      "    Validation accuracy: 0.95\n",
      "\n",
      "Starting with epoch 5...\n",
      "    Train accuracy: 0.96\n",
      "    Validation accuracy: 0.95\n",
      "\n",
      "Starting with epoch 6...\n",
      "    Train accuracy: 0.96\n",
      "    Validation accuracy: 0.96\n",
      "\n",
      "Starting with epoch 7...\n",
      "    Train accuracy: 0.96\n",
      "    Validation accuracy: 0.96\n",
      "\n",
      "Starting with epoch 8...\n",
      "    Train accuracy: 0.97\n",
      "    Validation accuracy: 0.96\n",
      "\n",
      "Starting with epoch 9...\n",
      "    Train accuracy: 0.97\n",
      "    Validation accuracy: 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_model(model, train_images, train_labels, validation_images, validation_labels, n_epochs=10, learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved quite a high accuracy, particularly compared to our baseline of about 82% from using the mean images as templates. It's also notable that there is close to no gap between the training and validation accuracy. That means we didn't overfit the dataset. This wouldn't have been probably anyway, given the low capacity and small size of our model. \n",
    "\n",
    "Let's take a look at some of the images we failed to classify correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: tensor([9, 3, 6, 4, 9], device='cuda:0')\n",
      "Guess: tensor([1, 7, 1, 9, 5], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def plot_images(images, nrows=None, ncols=None):\n",
    "    if ncols is None:\n",
    "        ncols = min(len(images), 5)\n",
    "    if nrows is None:\n",
    "        nrows = math.ceil(len(images) / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 8))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        image = images[i].squeeze()\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    validation_out, _ = model.forward(validation_images)\n",
    "    validation_guess = torch.argmax(validation_out, dim=-1)\n",
    "\n",
    "    wrong_images = validation_images[validation_guess != validation_labels]\n",
    "    N_wrong = wrong_images.shape[0]\n",
    "    wrong_images = wrong_images.reshape((N_wrong, 28, 28))\n",
    "    wrong_labels = validation_labels[validation_guess != validation_labels]\n",
    "    wrong_guess = validation_guess[validation_guess != validation_labels]\n",
    "\n",
    "    wrong_images = wrong_images.to('cpu')\n",
    "\n",
    "perm = torch.randperm(N_wrong)\n",
    "plot_images(wrong_images[perm[:5]])\n",
    "print(f'Labels: {wrong_labels[perm[:5]]}')\n",
    "print(f'Guess: {wrong_guess[perm[:5]]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the missclassifications you see might be really hard to tell, as the dataset contains some ambiguous numbers. Some of them might be easy to classify correctly for you as a human. A feedforward neural network isn't the typical architecture for image classification anyway (even though it worked quite well on these size-normalized, centered images). Architectures that are more typically used are Convolutional Neural Networks and Image Transformers. We won't go into ConvNets in this series, but Image Transformers use the same attention mechanism we will implement in the next chapter as their core building block. \n",
    "\n",
    "To close off with this section, let's see how we could have avoided doing the backwards calculation by ourselves with `torch.autograd`. Carefully read through the following notebook we implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch for training\n",
    "data_batch = train_images[:16,...]\n",
    "label_batch = train_labels[:16, ...]\n",
    "\n",
    "# Zero out the gradients for all parameters and telling torch, they need a gradient\n",
    "for key, param in model.params.items():\n",
    "    param.grad = None\n",
    "    param.requires_grad = True\n",
    "\n",
    "# The forward pass goes along as usual\n",
    "out, cache = model.forward(data_batch)\n",
    "\n",
    "# Loss calculation \n",
    "loss, dout = l2_loss(out, label_batch)\n",
    "\n",
    "# This is the pytorch magic: When calling backward on a single-element tensor,\n",
    "#  it will automatically compute the gradients for all tensors in the computational graph\n",
    "#  that require a gradient calculation.\n",
    "loss.backward()\n",
    "\n",
    "# Collecting the gradients calculated by autograd\n",
    "torch_grads = {\n",
    "    key: param.grad for key, param in model.params.items()\n",
    "}\n",
    "\n",
    "# Computing our own gradients for comparison\n",
    "with torch.no_grad():\n",
    "    model.backward(dout, cache)\n",
    "\n",
    "# Comparing our gradients with the model's gradients. The relative errors should be around 1e-6 \n",
    "\n",
    "    for key, grad in model.grads.items():\n",
    "        rel_err = (grad-torch_grads[key]).abs() / (1e-8+torch_grads[key].abs())\n",
    "        mean_rel_err = torch.mean(rel_err).item()\n",
    "        print(f'Relative error on {key}: {mean_rel_err}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this chapter, we have seen the basic mechanism of building and training a neural network. This section was kept really short, as we won't have to deal with training ourselves in our AlphaFold implementation. Training a model as big as AlphaFold would be infeasible on personal machines, and renting compute on this scale is really expensive. Also, our code won't be close to being optimized enough to allow for training it (even though theoretically, with torch.autograd, our implementation of the forward pass would allow us to do the full training). \n",
    "\n",
    "Still, there is so much more to discover in machine learning and training neural networks. For example, our training using stochastic gradient descent would typically be replaced by another optimization like Nesterov Momentum or Adam (both have the effect of putting more emphasis on weight changes that are consistent, and that aren't just reverted in the next batch). AlphaFold for example uses Adam for optimization during training. Also, there are many more layers that are really useful. The AlphaFold architecture for example uses dropout (randomly setting a set of values to zero during training, no effect during inference) and LayerNormalization (shifting and scaling the activations of a layer so that they have mean 0 and std 1). \n",
    "\n",
    "We won't implement any more of these here, but there are great resources online. If you still have Machine-Learning energy left after or during this series, check out the CS231n course from Stanford. The assignments from their offering in 2023 are freely available [here](https://github.com/cs231n/cs231n.github.io/tree/master/assignments/2023). You can find lecture slides and videos from previous offerings online. Assignment 1 is basically an improved and more extensive version of this notebook. More recent videos with similar topics were uploaded by Justin Johnson from the University of Michigan [here](https://youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&si=fCgVj8DhU1kTidVN). Both are great for getting a deeper introduction to machine learning, specifically Computer Vision.\n",
    "\n",
    "For now, we will leave it here and go on to our next, shorter chapter, where we will implement our first primitive for AlphaFold: MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphafold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

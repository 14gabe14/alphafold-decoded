{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "of_basic_features = torch.load('current_implementation/test_outputs/basic_features.pt', map_location='cpu')\n",
    "of_extra_msa_feat = torch.load('current_implementation/test_outputs/extra_msa_feat.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target_feat\n",
    "This is a feature of size [$N_{res}$ , 21] consisting of the “aatype” feature.\n",
    "- One-hot representation of the input amino acid sequence (20 amino\n",
    "acids + unknown).\n",
    "This feature is padded in openfold in data_transforms: make_msa_feat(), so it has shape [$N_{res}$, 22]\n",
    "For now, we simulate this with an additional letter Z in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK'\n",
    "\n",
    "\n",
    "def calculate_target_feat(sequence):\n",
    "    aa_codes = \"ZARNDCQEGHILKMFPSTWYVX\"\n",
    "    sequence_inds = torch.tensor([aa_codes.index(a) for a in sequence])\n",
    "    encoding = nn.functional.one_hot(sequence_inds, num_classes=22)\n",
    "    return encoding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### residue_index\n",
    "This is a feature of size [$N_{res}$] consisting of the \"residue_index\" feature\n",
    "- index into the original amino acid sequence, in our case just [0, ..., $N_{res}$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_residue_index(sequence):\n",
    "    return torch.arange(len(sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### msa_feat\n",
    "This is a feature of size [$N_{clust}$, $N_{res}$, 49] constructed by concatenating \"cluster_msa\", \"cluster_has_deletion\", \"cluster_deletion_value\", \"cluster_deletion_mean\", \"cluster_profile\".\n",
    "- This feature seems to be sampled randomly during recycling, this isn't implemented yet\n",
    "- \"cluster_msa\" of shape [$N_{clust}$, $N_{res}$, 23] is a one-hot representation of the cluster centers.\n",
    "- \"cluster_has_deletion\" of shape [$N_{clust}$, $N_{res}$, 1] is a binary feature indicating if there is a deletion to the left of the residue in the MSA cluster centres\n",
    "- \"cluster_deletion_value\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from current_implementation.structure_module import residue_constants\n",
    "\n",
    "def load_a3m_file(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    description_line_indices = [i for i,l in enumerate(lines) if l.startswith('>')]\n",
    "    descriptions = [lines[i].strip() for i in description_line_indices]\n",
    "    seqs = [lines[i+1].strip() for i in description_line_indices]\n",
    "    unique_seqs = []\n",
    "\n",
    "    deletion_count_matrix = []\n",
    "    for seq in seqs:\n",
    "        deletion_count_list = []\n",
    "        deletion_counter = 0\n",
    "        for letter in seq:\n",
    "            if letter.islower():\n",
    "                deletion_counter += 1\n",
    "            else:\n",
    "                deletion_count_list.append(deletion_counter)\n",
    "                deletion_counter = 0\n",
    "        seq_without_deletion = re.sub('[a-z]', '', seq)\n",
    "        if seq_without_deletion in unique_seqs:\n",
    "            continue\n",
    "        \n",
    "        unique_seqs.append(seq_without_deletion)\n",
    "        deletion_count_matrix.append(deletion_count_list)\n",
    "\n",
    "    aa_codes = \"ARNDCQEGHILKMFPSTWYVX-\"\n",
    "    unique_seqs = torch.stack([\n",
    "            torch.tensor([aa_codes.index(a) for a in seq]) \n",
    "        for seq in unique_seqs])\n",
    "    unique_seqs_one_hot = nn.functional.one_hot(unique_seqs, num_classes=22)\n",
    "    amino_acid_distribution = unique_seqs_one_hot.float().mean(dim=0)\n",
    "    deletion_count_matrix = torch.tensor(deletion_count_matrix)\n",
    "    \n",
    "    return { 'msa_aatype': unique_seqs, 'msa_deletion_count': deletion_count_matrix, 'amino_acid_distribution': amino_acid_distribution}\n",
    "\n",
    "# features = load_a3m_file('kilian/alignments_hhr/test_tautomerase/test_tautomerase.a3m')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clusters(features, max_msa_clusters=512, basic_seed=0):\n",
    "    sequences = features['msa_aatype']\n",
    "    num_seqs = sequences.shape[0]\n",
    "    max_msa_clusters = min(max_msa_clusters, num_seqs)\n",
    "    random_generator = torch.Generator(device=sequences.device)\n",
    "    random_generator.manual_seed(basic_seed)\n",
    "    shuffled = torch.randperm(num_seqs-1, generator=random_generator)+1\n",
    "    shuffled = torch.cat((torch.tensor([0]), shuffled), dim=0)\n",
    "\n",
    "    resulting_features = {key: value.clone() for key, value in features.items()}\n",
    "    MSA_FEATURE_NAMES = ['msa_aatype', 'msa_deletion_count']\n",
    "    for key in MSA_FEATURE_NAMES:\n",
    "        if key not in features:\n",
    "            continue\n",
    "        feature = features[key]\n",
    "        resulting_features = {\n",
    "            **resulting_features, \n",
    "            key: feature[shuffled[:max_msa_clusters]],\n",
    "            f'extra_{key}': feature[shuffled[max_msa_clusters:]] \n",
    "        }\n",
    "    return resulting_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributions as distributions\n",
    "def mask_clusters(features, mask_probability=0.15, basic_seed=0):\n",
    "    features = {key: value.clone() for key, value in features.items()}\n",
    "    N_clust, N_res = features['msa_aatype'].shape\n",
    "    N_aa_categories = 23 # 20 Amino Acids, Unknown Amino Acid, Gap, masked_msa_token\n",
    "    odds = {\n",
    "        'uniform_replacement': 0.1,\n",
    "        'replacement_from_distribution': 0.1,\n",
    "        'no_replacement': 0.1,\n",
    "        'masked_out': 0.7\n",
    "    }\n",
    "    uniform_category = torch.tensor([1/20] * 20 + [0, 0]) * odds['uniform_replacement']\n",
    "    replacement_from_distribution = features['amino_acid_distribution'] * odds['replacement_from_distribution']\n",
    "    no_replacement = nn.functional.one_hot(features['msa_aatype'], num_classes=22) * odds['no_replacement']\n",
    "    masked_out = torch.tensor([odds['masked_out']]).expand((N_clust, N_res, 1))\n",
    "\n",
    "    transition_categories_without_mask = uniform_category + replacement_from_distribution + no_replacement\n",
    "    transition_categories = torch.cat((transition_categories_without_mask, masked_out), dim=-1)\n",
    "    unk_in_msa = features['msa_aatype'] == 20\n",
    "\n",
    "\n",
    "    gen = torch.Generator(device=features['msa_aatype'].device)\n",
    "    gen.manual_seed(basic_seed+1)\n",
    "    mask = torch.rand(features['msa_aatype'].shape, generator=gen) < mask_probability\n",
    "\n",
    "    torch.manual_seed(basic_seed+1)\n",
    "    replacement = distributions.Categorical(probs=transition_categories.reshape(-1, N_aa_categories)).sample()\n",
    "    replacement = replacement.reshape(N_clust, N_res)\n",
    "    features['true_msa_aatype'] = features['msa_aatype'].clone()\n",
    "    features['msa_aatype'][mask] = replacement[mask]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_assignment(features):\n",
    "    N_clust, N_res = features['msa_aatype'].shape\n",
    "    N_extra, _ = features['extra_msa_aatype'].shape\n",
    "\n",
    "    msa_broadcast = features['msa_aatype'].reshape((N_clust, N_res, 1))\n",
    "    extra_broadcast = features['extra_msa_aatype'].T.reshape((1, N_res, N_extra))\n",
    "    mask = torch.logical_and(msa_broadcast != 22, msa_broadcast != 21)\n",
    "    agreement = torch.logical_and(msa_broadcast == extra_broadcast, mask).sum(dim=1)\n",
    "    assignment = torch.argmax(agreement, dim=0)\n",
    "    features['extra_cluster_assignment'] = assignment\n",
    "           \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_clusters(features):\n",
    "    N_clust, N_res = features['msa_aatype'].shape\n",
    "    N_extra, _ = features['extra_msa_aatype'].shape\n",
    "    assignment = features['extra_cluster_assignment']\n",
    "    assignment_counts = (torch.arange(N_clust).reshape(N_clust, 1) == assignment.reshape(1, N_extra)).sum(dim=1) + 1\n",
    "\n",
    "    def cluster_average(feature, extra_feature):\n",
    "        unsqueezed_shape = [-1] + [1] * (extra_feature.ndim-1)\n",
    "        broadcast_assignment = assignment.reshape(unsqueezed_shape).expand(extra_feature.shape)\n",
    "        result = torch.scatter_add(feature, 0, broadcast_assignment, extra_feature)\n",
    "        return result / assignment_counts.reshape(unsqueezed_shape)\n",
    "\n",
    "    cluster_deletion_mean = cluster_average(features['msa_deletion_count'], features['extra_msa_deletion_count'])\n",
    "    cluster_deletion_mean = 2/torch.pi * torch.arctan(cluster_deletion_mean / 3)\n",
    "\n",
    "    msa_one_hot = nn.functional.one_hot(features['msa_aatype'], num_classes=23)\n",
    "    extra_msa_one_hot = nn.functional.one_hot(features['extra_msa_aatype'], num_classes=23)\n",
    "    cluster_profile = cluster_average(msa_one_hot, extra_msa_one_hot)\n",
    "\n",
    "    features['cluster_deletion_mean'] = cluster_deletion_mean\n",
    "    features['cluster_profile'] = cluster_profile\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_extra_msa(features, max_extra_msa_count=5120, basic_seed=0):\n",
    "    N_extra = features['extra_msa_aatype'].shape[0]\n",
    "    gen = torch.Generator(features['extra_msa_aatype'].device)\n",
    "    gen.manual_seed(basic_seed+2)\n",
    "    inds_to_select = torch.randperm(N_extra, generator=gen)[:max_extra_msa_count]\n",
    "    for k,v in features.items():\n",
    "        if 'extra_' in k:\n",
    "            features[k] = features[k][inds_to_select]\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_msa_feat(features):\n",
    "    N_clust, N_res = features['msa_aatype'].shape\n",
    "\n",
    "    cluster_msa = nn.functional.one_hot(features['msa_aatype'], num_classes=23)\n",
    "\n",
    "    cluster_has_deletion = (features['msa_deletion_count'] > 0).float()\n",
    "    cluster_has_deletion = cluster_has_deletion.reshape(N_clust, N_res, 1)\n",
    "\n",
    "    cluster_deletion_value = 2/torch.pi * torch.arctan(features['msa_deletion_count'] / 3)\n",
    "    cluster_deletion_value = cluster_deletion_value.reshape(N_clust, N_res, 1)\n",
    "\n",
    "    cluster_deletion_mean = features['cluster_deletion_mean']\n",
    "    cluster_deletion_mean = cluster_deletion_mean.reshape(N_clust, N_res, 1)\n",
    "    cluster_profile = features['cluster_profile']\n",
    "\n",
    "    msa_feat = torch.cat((cluster_msa, cluster_has_deletion, cluster_deletion_value, cluster_profile, cluster_deletion_mean), dim=-1)\n",
    "    return msa_feat\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_extra_msa_feat(features):\n",
    "    N_extra, N_res = features['extra_msa_aatype'].shape\n",
    "\n",
    "    extra_msa = nn.functional.one_hot(features['extra_msa_aatype'], num_classes=23)\n",
    "    extra_msa_has_deletion = (features['extra_msa_deletion_count'] > 0).float()\n",
    "    extra_msa_has_deletion = extra_msa_has_deletion.reshape(N_extra, N_res, 1)\n",
    "    extra_msa_deletion_value = 2/torch.pi * torch.arctan(features['extra_msa_deletion_count']/3)\n",
    "    extra_msa_deletion_value = extra_msa_deletion_value.reshape(N_extra, N_res, 1)\n",
    "\n",
    "    return torch.cat((extra_msa, extra_msa_has_deletion, extra_msa_deletion_value), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "msa_aatype\n",
      "msa_deletion_count\n",
      "extra_msa_aatype\n",
      "extra_msa_deletion_count\n",
      "true_msa_aatype\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Error with extra_msa_aatype after cropping.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra_cluster_assignment\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     48\u001b[0m             key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_assignment\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(param\u001b[38;5;241m.\u001b[39mfloat(), other_features[key]\u001b[38;5;241m.\u001b[39mfloat()), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after cropping.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     50\u001b[0m msa_feat \u001b[38;5;241m=\u001b[39m calculate_msa_feat(features)\n\u001b[1;32m     51\u001b[0m extra_msa_feat \u001b[38;5;241m=\u001b[39m calculate_extra_msa_feat(features)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Error with extra_msa_aatype after cropping."
     ]
    }
   ],
   "source": [
    "seq_data = load_a3m_file('current_implementation/alignments_hhr/test_tautomerase/test_tautomerase.a3m')\n",
    "msa_feats = []\n",
    "extra_msa_feats = []\n",
    "for i in range(4):\n",
    "    print(f'Iteration {i}')\n",
    "    features = split_clusters(seq_data, basic_seed=i)\n",
    "    other_features = torch.load('solutions/feature_extraction/control_values/clusters_selected.pt')\n",
    "    for key, param in features.items():\n",
    "        if key != 'amino_acid_distribution':\n",
    "            if 'aatype' in key:\n",
    "                other_features[key] = other_features[key].argmax(dim=-1)\n",
    "            assert torch.allclose(param.float(), other_features[key].float()), f'Error with {key} after splitting.'\n",
    "    features = mask_clusters(features, basic_seed=i)\n",
    "    other_features = torch.load('solutions/feature_extraction/control_values/clusters_masked.pt')\n",
    "    for key, param in features.items():\n",
    "        if key != 'amino_acid_distribution':\n",
    "            if 'aatype' in key:\n",
    "                other_features[key] = other_features[key].argmax(dim=-1)\n",
    "            print(key)\n",
    "            assert torch.allclose(param.float(), other_features[key].float()), f'Error with {key} after masking.'\n",
    "    features = cluster_assignment(features)\n",
    "    other_features = torch.load('solutions/feature_extraction/control_values/clusters_assigned.pt')\n",
    "    for key, param in features.items():\n",
    "        if key != 'amino_acid_distribution':\n",
    "            if 'aatype' in key:\n",
    "                other_features[key] = other_features[key].argmax(dim=-1)\n",
    "\n",
    "            if key == 'extra_cluster_assignment':\n",
    "                key = 'cluster_assignment'\n",
    "            assert torch.allclose(param.float(), other_features[key].float()), f'Error with {key} after assigning.'\n",
    "    features = summarize_clusters(features)\n",
    "    other_features = torch.load('solutions/feature_extraction/control_values/clusters_summarized.pt')\n",
    "    for key, param in features.items():\n",
    "        if key != 'amino_acid_distribution':\n",
    "            if 'aatype' in key:\n",
    "                other_features[key] = other_features[key].argmax(dim=-1)\n",
    "            if key == 'extra_cluster_assignment':\n",
    "                key = 'cluster_assignment'\n",
    "            assert torch.allclose(param.float(), other_features[key].float()), f'Error with {key} after summarizing.'\n",
    "    features = crop_extra_msa(features, basic_seed=i)\n",
    "    other_features = torch.load('solutions/feature_extraction/control_values/extra_msa_cropped.pt')\n",
    "    for key, param in features.items():\n",
    "        if key != 'amino_acid_distribution':\n",
    "            if 'aatype' in key:\n",
    "                other_features[key] = other_features[key].argmax(dim=-1)\n",
    "\n",
    "            if key == 'extra_cluster_assignment':\n",
    "                key = 'cluster_assignment'\n",
    "            assert torch.allclose(param.float(), other_features[key].float()), f'Error with {key} after cropping.'\n",
    "    msa_feat = calculate_msa_feat(features)\n",
    "    extra_msa_feat = calculate_extra_msa_feat(features)\n",
    "\n",
    "    msa_feats.append(msa_feat)\n",
    "    extra_msa_feats.append(extra_msa_feat)\n",
    "\n",
    "target_feat = calculate_target_feat(sequence)\n",
    "residue_index = calculate_residue_index(sequence)\n",
    "\n",
    "msa_feat = torch.stack(msa_feats, dim=-1)\n",
    "extra_msa_feat = torch.stack(extra_msa_feats, dim=-1)\n",
    "target_feat = target_feat.view(target_feat.shape+(1,)).broadcast_to(target_feat.shape+(4,))\n",
    "residue_index = residue_index.view(residue_index.shape+(1,)).broadcast_to(residue_index.shape+(4,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msa_feat:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (49) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsa_feat:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[43mmsa_feat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mof_basic_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmsa_feat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_feat:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m((target_feat \u001b[38;5;241m-\u001b[39m of_basic_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_feat\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (49) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "print('msa_feat:')\n",
    "print((msa_feat - of_basic_features['msa_feat']).abs().mean())\n",
    "print('target_feat:')\n",
    "print((target_feat - of_basic_features['target_feat']).abs().mean())\n",
    "print('residue index:')\n",
    "print((residue_index - of_basic_features['residue_index']).float().abs().mean())\n",
    "print('extra_msa_feat:')\n",
    "print((extra_msa_feat[...,0] - of_extra_msa_feat).abs().mean())\n",
    "\n",
    "my_batch = {\n",
    "    'msa_feat': msa_feat.float(),\n",
    "    'extra_msa_feat': extra_msa_feat.float(),\n",
    "    'target_feat': target_feat.float(),\n",
    "    'residue_index': residue_index.float()\n",
    "}\n",
    "\n",
    "# torch.save(my_batch, 'kilian/my_batch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openfold_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
